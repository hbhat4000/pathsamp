
@misc{center_for_history_and_new_media_zotero_nodate,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}}
}

@article{cannon_quantile_2011,
	title = {Quantile regression neural networks: {Implementation} in {R} and application to precipitation downscaling},
	volume = {37},
	issn = {00983004},
	shorttitle = {Quantile regression neural networks},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S009830041000292X},
	doi = {10.1016/j.cageo.2010.07.005},
	language = {en},
	number = {9},
	urldate = {2015-07-08},
	journal = {Computers \& Geosciences},
	author = {Cannon, Alex J.},
	month = sep,
	year = {2011},
	pages = {1277--1284},
}

@article{chang_multi-step_2014,
	title = {Multi-step quantile regression tree},
	volume = {84},
	issn = {0094-9655, 1563-5163},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00949655.2012.721886},
	doi = {10.1080/00949655.2012.721886},
	language = {en},
	number = {3},
	urldate = {2015-07-08},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Chang, Youngjae},
	month = mar,
	year = {2014},
	pages = {663--682},
}

@article{ciuperca_penalized_2011,
	title = {Penalized least absolute deviations estimation for nonlinear model with change-points},
	volume = {52},
	issn = {0932-5026, 1613-9798},
	url = {http://link.springer.com/10.1007/s00362-009-0236-6},
	doi = {10.1007/s00362-009-0236-6},
	language = {en},
	number = {2},
	urldate = {2015-07-08},
	journal = {Statistical Papers},
	author = {Ciuperca, Gabriela},
	month = may,
	year = {2011},
	pages = {371--390},
}

@article{koenker_additive_2011,
	title = {Additive models for quantile regression: {Model} selection and confidence bandaids},
	volume = {25},
	issn = {0103-0752},
	shorttitle = {Additive models for quantile regression},
	url = {http://projecteuclid.org/euclid.bjps/1313973394},
	doi = {10.1214/10-BJPS131},
	language = {en},
	number = {3},
	urldate = {2015-07-08},
	journal = {Brazilian Journal of Probability and Statistics},
	author = {Koenker, Roger},
	month = nov,
	year = {2011},
	pages = {239--262},
}

@article{giloni_robust_2006,
	title = {Robust weighted {LAD} regression},
	volume = {50},
	issn = {01679473},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947305001325},
	doi = {10.1016/j.csda.2005.06.005},
	language = {en},
	number = {11},
	urldate = {2015-07-08},
	journal = {Computational Statistics \& Data Analysis},
	author = {Giloni, Avi and Simonoff, Jeffrey S. and Sengupta, Bhaskar},
	month = jul,
	year = {2006},
	pages = {3124--3140},
}

@article{spokoiny_local_2013,
	title = {Local quantile regression},
	volume = {143},
	issn = {03783758},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0378375813000530},
	doi = {10.1016/j.jspi.2013.03.008},
	language = {en},
	number = {7},
	urldate = {2015-07-08},
	journal = {Journal of Statistical Planning and Inference},
	author = {Spokoiny, Vladimir and Wang, Weining and Karl Härdle, Wolfgang},
	month = jul,
	year = {2013},
	pages = {1109--1129},
}

@article{dielman_least_2005,
	title = {Least absolute value regression: recent contributions},
	volume = {75},
	issn = {0094-9655, 1563-5163},
	shorttitle = {Least absolute value regression},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0094965042000223680},
	doi = {10.1080/0094965042000223680},
	language = {en},
	number = {4},
	urldate = {2015-07-08},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Dielman, Terry E.},
	month = apr,
	year = {2005},
	pages = {263--286},
}

@article{ciuperca_estimating_2011,
	title = {Estimating nonlinear regression with and without change-points by the {LAD} method},
	volume = {63},
	issn = {0020-3157, 1572-9052},
	url = {http://link.springer.com/10.1007/s10463-009-0256-y},
	doi = {10.1007/s10463-009-0256-y},
	language = {en},
	number = {4},
	urldate = {2015-07-08},
	journal = {Annals of the Institute of Statistical Mathematics},
	author = {Ciuperca, Gabriela},
	month = aug,
	year = {2011},
	pages = {717--743},
}

@article{loh_classification_2011,
	title = {Classification and regression trees},
	volume = {1},
	issn = {19424787},
	url = {http://doi.wiley.com/10.1002/widm.8},
	doi = {10.1002/widm.8},
	language = {en},
	number = {1},
	urldate = {2015-07-08},
	journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	author = {Loh, Wei-Yin},
	month = jan,
	year = {2011},
	pages = {14--23},
}

@article{loh_fifty_2014,
	title = {Fifty {Years} of {Classification} and {Regression} {Trees}: {Fifty} {Years} of {Classification} and {Regression} {Trees}},
	volume = {82},
	issn = {03067734},
	shorttitle = {Fifty {Years} of {Classification} and {Regression} {Trees}},
	url = {http://doi.wiley.com/10.1111/insr.12016},
	doi = {10.1111/insr.12016},
	language = {en},
	number = {3},
	urldate = {2015-07-08},
	journal = {International Statistical Review},
	author = {Loh, Wei-Yin},
	month = dec,
	year = {2014},
	pages = {329--348},
}

@phdthesis{torgo_inductive_1999,
	title = {Inductive learning of tree-based regression models},
	url = {http://repositorio-aberto.up.pt/handle/10216/10018},
	urldate = {2015-07-08},
	school = {Universidade do Porto},
	author = {Torgo, Luís Fernando Raínho Alves},
	year = {1999},
}

@book{bloomfield_least_1983,
	series = {Progress in {Probability} and {Statistics}},
	title = {Least absolute deviations},
	volume = {6},
	isbn = {0-8176-3157-7},
	url = {http://www.ams.org/mathscinet-getitem?mr=748483},
	urldate = {2015-07-08},
	publisher = {Birkhäuser Boston, Inc., Boston, MA},
	author = {Bloomfield, Peter and Steiger, William L.},
	year = {1983},
	mrnumber = {748483},
	note = {Theory, applications, and algorithms}
}

@misc{noauthor_10algorithms-08.pdf_nodate,
	title = {10Algorithms-08.pdf},
}

@article{wu_top_2008,
	title = {Top 10 algorithms in data mining},
	volume = {14},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-007-0114-2},
	doi = {10.1007/s10115-007-0114-2},
	language = {en},
	number = {1},
	urldate = {2015-07-08},
	journal = {Knowledge and Information Systems},
	author = {Wu, Xindong and Kumar, Vipin and Ross Quinlan, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
	month = jan,
	year = {2008},
	pages = {1--37},
}

@article{zhou_quantile_2014,
	title = {Quantile {Regression} via the {EM} {Algorithm}},
	volume = {43},
	issn = {0361-0918, 1532-4141},
	url = {http://www.tandfonline.com/doi/abs/10.1080/03610918.2012.746980},
	doi = {10.1080/03610918.2012.746980},
	language = {en},
	number = {10},
	urldate = {2015-07-09},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {Zhou, Ying-hui and Ni, Zhong-xin and Li, Yong},
	month = nov,
	year = {2014},
	pages = {2162--2172},
}

@article{schon_probabilistic_2017,
	title = {Probabilistic learning of nonlinear dynamical systems using sequential {Monte} {Carlo}},
	url = {https://arxiv.org/abs/1703.02419},
	urldate = {2017-03-15},
	author = {Schön, Thomas B. and Svensson, Andreas and Murray, Lawrence and Lindsten, Fredrik},
	year = {2017},
}

@article{schaeffer_learning_2017,
	title = {Learning partial differential equations via data discovery and sparse optimization},
	volume = {473},
	doi = {10.1098/rspa.2016.0446},
	language = {en},
	number = {2197},
	urldate = {2017-03-15},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science},
	author = {Schaeffer, Hayden},
	month = jan,
	year = {2017},
	pages = {20160446},
}

@inproceedings{belanger_linear_2015,
	title = {A {Linear} {Dynamical} {System} {Model} for {Text}.},
	url = {http://www.jmlr.org/proceedings/papers/v37/belanger15.pdf},
	urldate = {2017-03-15},
	booktitle = {{ICML}},
	author = {Belanger, David and Kakade, Sham M.},
	year = {2015},
	pages = {833--842},
}

@article{boccignone_probabilistic_2016,
	title = {A probabilistic tour of visual attention and gaze shift computational models},
	url = {https://arxiv.org/abs/1607.01232},
	urldate = {2017-03-15},
	journal = {arXiv preprint arXiv:1607.01232},
	author = {Boccignone, Giuseppe},
	year = {2016},
}

@article{brillinger_employing_2002,
	title = {Employing stochastic differential equations to model wildlife motion},
	volume = {33},
	url = {http://link.springer.com/article/10.1007/s005740200021},
	number = {3},
	urldate = {2017-03-15},
	journal = {Bulletin of the Brazilian Mathematical Society},
	author = {Brillinger, David R. and Preisler, Haiganoush K. and Ager, Alan A. and Kie, John G. and Stewart, Brent S.},
	year = {2002},
	pages = {385--408},
}

@phdthesis{fox_bayesian_2009,
	title = {Bayesian nonparametric learning of complex dynamical phenomena},
	url = {http://people.csail.mit.edu/fisher/publications/theses/Fox_PhDThesis09.pdf},
	urldate = {2017-03-15},
	school = {Massachusetts Institute of Technology},
	author = {Fox, Emily B.},
	year = {2009},
}

@article{fox_bayesian_2011,
	title = {Bayesian {Nonparametric} {Inference} of {Switching} {Dynamic} {Linear} {Models}},
	volume = {59},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/5680657/},
	doi = {10.1109/TSP.2010.2102756},
	number = {4},
	urldate = {2017-03-15},
	journal = {IEEE Transactions on Signal Processing},
	author = {Fox, Emily and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
	month = apr,
	year = {2011},
	pages = {1569--1585},
}

@inproceedings{fox_nonparametric_2009,
	title = {Nonparametric {Bayesian} learning of switching linear dynamical systems},
	url = {http://papers.nips.cc/paper/3546-nonparametric-bayesian-learning-of-switching-linear-dynamical-systems},
	urldate = {2017-03-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Fox, Emily and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
	year = {2009},
	pages = {457--464},
}

@article{bongard_automated_2007,
	title = {Automated reverse engineering of nonlinear dynamical systems},
	volume = {104},
	url = {http://www.pnas.org/content/104/24/9943.short},
	number = {24},
	urldate = {2017-03-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bongard, Josh and Lipson, Hod},
	year = {2007},
	pages = {9943--9948},
}

@article{schaeffer_sparse_2013,
	title = {Sparse dynamics for partial differential equations},
	volume = {110},
	doi = {10.1073/pnas.1302752110},
	language = {en},
	number = {17},
	urldate = {2017-03-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Schaeffer, H. and Caflisch, R. and Hauck, C. D. and Osher, S.},
	month = apr,
	year = {2013},
	pages = {6634--6639},
}

@article{brunton_discovering_2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	doi = {10.1073/pnas.1517384113},
	language = {en},
	number = {15},
	urldate = {2017-03-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2016},
	pages = {3932--3937},
}

@article{oja_simplified_1982,
	title = {Simplified neuron model as a principal component analyzer},
	volume = {15},
	url = {http://www.springerlink.com/index/u9u6120r003825u1.pdf},
	number = {3},
	urldate = {2017-03-15},
	journal = {Journal of Mathematical Biology},
	author = {Oja, Erkki},
	year = {1982},
	pages = {267--273},
}

@article{martens_calculation_2012,
	title = {Calculation of high-dimensional probability density functions of stochastically excited nonlinear mechanical systems},
	volume = {67},
	issn = {0924-090X, 1573-269X},
	url = {http://link.springer.com/10.1007/s11071-011-0131-2},
	doi = {10.1007/s11071-011-0131-2},
	language = {en},
	number = {3},
	urldate = {2017-03-15},
	journal = {Nonlinear Dynamics},
	author = {Martens, Wolfram and von Wagner, Utz and Mehrmann, Volker},
	month = feb,
	year = {2012},
	pages = {2089--2099},
}

@article{ghahramani_learning_1999,
	title = {Learning nonlinear dynamical systems using an {EM} algorithm},
	urldate = {2017-03-15},
	journal = {Advances in Neural Information Processing Systems (NIPS)},
	author = {Ghahramani, Zoubin and Roweis, Sam T.},
	year = {1999},
	pages = {431--437},
}

@article{khansari-zadeh_learning_2011,
	title = {Learning {Stable} {Nonlinear} {Dynamical} {Systems} {With} {Gaussian} {Mixture} {Models}},
	volume = {27},
	issn = {1552-3098, 1941-0468},
	url = {http://ieeexplore.ieee.org/document/5953529/},
	doi = {10.1109/TRO.2011.2159412},
	number = {5},
	urldate = {2017-03-15},
	journal = {IEEE Transactions on Robotics},
	author = {Khansari-Zadeh, S. Mohammad and Billard, Aude},
	month = oct,
	year = {2011},
	pages = {943--957},
}

@article{nicolau_new_2002,
	title = {A new technique for simulating the likelihood of stochastic differential equations},
	volume = {5},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/1368-423X.t01-1-00075/full},
	number = {1},
	urldate = {2017-03-15},
	journal = {The Econometrics Journal},
	author = {Nicolau, João},
	year = {2002},
	pages = {91--103},
}

@article{daniels_automated_2015,
	title = {Automated adaptive inference of phenomenological dynamical models},
	volume = {6},
	issn = {2041-1723},
	url = {http://www.nature.com/doifinder/10.1038/ncomms9133},
	doi = {10.1038/ncomms9133},
	urldate = {2017-03-15},
	journal = {Nature Communications},
	author = {Daniels, Bryan C. and Nemenman, Ilya},
	month = aug,
	year = {2015},
	pages = {8133},
}

@article{klimovskaia_sparse_2016,
	title = {Sparse {Regression} {Based} {Structure} {Learning} of {Stochastic} {Reaction} {Networks} from {Single} {Cell} {Snapshot} {Time} {Series}},
	volume = {12},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005234},
	number = {12},
	urldate = {2017-03-15},
	journal = {PLOS Computational Biology},
	author = {Klimovskaia, Anna and Ganscha, Stefan and Claassen, Manfred},
	year = {2016},
	pages = {e1005234},
}

@inproceedings{li_simultaneous_2007,
	title = {Simultaneous learning of nonlinear manifold and dynamical models for high-dimensional time series},
	url = {http://ieeexplore.ieee.org/abstract/document/4409044/},
	urldate = {2017-03-15},
	booktitle = {Computer {Vision}, 2007. {ICCV} 2007. {IEEE} 11th {International} {Conference} on},
	publisher = {IEEE},
	author = {Li, Rui and Tian, Tai-Peng and Sclaroff, Stan},
	year = {2007},
	pages = {1--8},
}

@inproceedings{pereira_learning_2010,
	title = {Learning networks of stochastic differential equations},
	url = {http://papers.nips.cc/paper/4055-learning-networks-of-stochastic-differential-equations},
	urldate = {2017-03-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Pereira, José and Ibrahimi, Morteza and Montanari, Andrea},
	year = {2010},
	pages = {172--180},
}

@article{genon-catalot_non-parametric_1992,
	title = {Non-parametric estimation of the diffusion coefficient by wavelets methods},
	url = {http://www.jstor.org/stable/4616250},
	urldate = {2017-03-15},
	journal = {Scandinavian Journal of Statistics},
	author = {Genon-Catalot, Valentine and Laredo, Catherine and Picard, Dominique},
	year = {1992},
	pages = {317--335},
}

@article{schmidt_distilling_2009,
	title = {Distilling free-form natural laws from experimental data},
	volume = {324},
	url = {http://science.sciencemag.org/content/324/5923/81.short},
	number = {5923},
	urldate = {2017-03-15},
	journal = {Science},
	author = {Schmidt, Michael and Lipson, Hod},
	year = {2009},
	pages = {81--85},
}

@article{papaspiliopoulos_nonparametric_2012,
	title = {Nonparametric estimation of diffusions: a differential equations approach},
	volume = {99},
	issn = {0006-3444, 1464-3510},
	shorttitle = {Nonparametric estimation of diffusions},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/ass034},
	doi = {10.1093/biomet/ass034},
	language = {en},
	number = {3},
	urldate = {2017-03-15},
	journal = {Biometrika},
	author = {Papaspiliopoulos, O. and Pokern, Y. and Roberts, G. O. and Stuart, A. M.},
	month = sep,
	year = {2012},
	pages = {511--531},
}

@article{muller_empirical_2010,
	title = {Empirical dynamics for longitudinal data},
	volume = {38},
	url = {http://projecteuclid.org/euclid.aos/1291126964},
	number = {6},
	urldate = {2017-03-15},
	journal = {The Annals of Statistics},
	author = {Müller, Hans-Georg and Yao, Fang and {others}},
	year = {2010},
	pages = {3458--3486},
}

@article{verzelen_inferring_2012,
	title = {Inferring stochastic dynamics from functional data},
	volume = {99},
	number = {3},
	urldate = {2017-03-15},
	journal = {Biometrika},
	author = {Verzelen, Nicolas and Tao, Wenwen and Müller, Hans-Georg and {others}},
	year = {2012},
	pages = {533--550},
}

@article{nicolau_nonparametric_2007,
	title = {Nonparametric estimation of second-order stochastic differential equations},
	volume = {23},
	doi = {10.1017/S0266466607070375},
	language = {en},
	number = {05},
	urldate = {2017-03-15},
	journal = {Econometric Theory},
	author = {Nicolau, João},
	month = oct,
	year = {2007},
	pages = {880},
}

@book{holtz_sparse_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computational} {Science} and {Engineering}},
	title = {Sparse {Grid} {Quadrature} in {High} {Dimensions} with {Applications} in {Finance} and {Insurance}},
	volume = {77},
	isbn = {978-3-642-16003-5 978-3-642-16004-2},
	url = {http://link.springer.com/10.1007/978-3-642-16004-2},
	urldate = {2017-03-15},
	publisher = {Springer Berlin Heidelberg},
	author = {Holtz, Markus},
	year = {2011},
	doi = {10.1007/978-3-642-16004-2},
}

@article{genz_fully_1996,
	title = {Fully symmetric interpolatory rules for multiple integrals over infinite regions with {Gaussian} weight},
	volume = {71},
	url = {http://www.sciencedirect.com/science/article/pii/0377042795002324},
	number = {2},
	urldate = {2017-03-15},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Genz, Alan and Keister, Bradley D.},
	year = {1996},
	pages = {299--309},
}

@article{heiss_likelihood_2008,
	title = {Likelihood approximation by numerical integration on sparse grids},
	volume = {144},
	issn = {03044076},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304407607002552},
	doi = {10.1016/j.jeconom.2007.12.004},
	language = {en},
	number = {1},
	urldate = {2017-03-15},
	journal = {Journal of Econometrics},
	author = {Heiss, Florian and Winschel, Viktor},
	month = may,
	year = {2008},
	pages = {62--80},
}

@article{koutsourelakis_scalable_2011,
	title = {Scalable {Bayesian} reduced-order models for simulating high-dimensional multiscale dynamical systems},
	volume = {9},
	url = {http://epubs.siam.org/doi/abs/10.1137/100783790},
	number = {1},
	urldate = {2017-03-16},
	journal = {Multiscale Modeling \& Simulation},
	author = {Koutsourelakis, Phaedon-Stelios and Bilionis, Elias},
	year = {2011},
	pages = {449--485},
}

@article{jankovic_new_2003,
	title = {A new simple ∞{OH} neuron model as a biologically plausible principal component analyzer},
	volume = {14},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/1215402/},
	doi = {10.1109/TNN.2003.813836},
	language = {en},
	number = {4},
	urldate = {2017-03-16},
	journal = {IEEE Transactions on Neural Networks},
	author = {Jankovic, M.V.},
	month = jul,
	year = {2003},
	pages = {853--859},
}

@article{horenko_automated_2008,
	title = {Automated {Generation} of {Reduced} {Stochastic} {Weather} {Models} {I}: {Simultaneous} {Dimension} and {Model} {Reduction} for {Time} {Series} {Analysis}},
	volume = {6},
	issn = {1540-3459, 1540-3467},
	shorttitle = {Automated {Generation} of {Reduced} {Stochastic} {Weather} {Models} {I}},
	url = {http://epubs.siam.org/doi/10.1137/060670535},
	doi = {10.1137/060670535},
	language = {en},
	number = {4},
	urldate = {2017-03-16},
	journal = {Multiscale Modeling \& Simulation},
	author = {Horenko, Illia and Klein, Rupert and Dolaptchiev, Stamen and Schütte, Christof},
	month = jan,
	year = {2008},
	pages = {1125--1145},
}

@article{freeman_mapping_2014,
	title = {Mapping brain activity at scale with cluster computing},
	volume = {11},
	issn = {1548-7091, 1548-7105},
	url = {http://www.nature.com/doifinder/10.1038/nmeth.3041},
	doi = {10.1038/nmeth.3041},
	number = {9},
	urldate = {2017-03-16},
	journal = {Nature Methods},
	author = {Freeman, Jeremy and Vladimirov, Nikita and Kawashima, Takashi and Mu, Yu and Sofroniew, Nicholas J and Bennett, Davis V and Rosen, Joshua and Yang, Chao-Tsung and Looger, Loren L and Ahrens, Misha B},
	month = jul,
	year = {2014},
	pages = {941--950},
}

@article{crutchfield_equations_1987,
	title = {Equations of motion from a data series},
	volume = {1},
	number = {417-452},
	journal = {Complex systems},
	author = {Crutchfield, James P and McNamara, Bruce S},
	year = {1987},
	pages = {121},
}

@article{batz_variational_2016,
	title = {Variational estimation of the drift for stochastic differential equations from the empirical density},
	volume = {2016},
	doi = {10.1088/1742-5468/2016/08/083404},
	number = {8},
	urldate = {2017-03-16},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Batz, Philipp and Ruttor, Andreas and Opper, Manfred},
	month = aug,
	year = {2016},
	pages = {083404},
}

@article{svensson_computationally_2015,
	title = {Computationally efficient {Bayesian} learning of {Gaussian} process state space models},
	url = {http://www.jmlr.org/proceedings/papers/v51/svensson16.pdf},
	urldate = {2017-03-16},
	journal = {arXiv preprint arXiv: 1506: 02267},
	author = {Svensson, Andreas and Solin, Arno and Särkkä, Simo and Schön, Thomas B.},
	year = {2015},
}

@article{batz_approximate_2017,
	title = {Approximate {Bayes} learning of stochastic differential equations},
	url = {https://arxiv.org/abs/1702.05390},
	urldate = {2017-03-16},
	journal = {arXiv preprint arXiv:1702.05390},
	author = {Batz, Philipp and Ruttor, Andreas and Opper, Manfred},
	year = {2017},
}

@inproceedings{wu_spectral_2016,
	title = {Spectral learning of dynamic systems from nonequilibrium data},
	url = {http://papers.nips.cc/paper/6191-spectral-learning-of-dynamic-systems-from-nonequilibrium-data},
	urldate = {2017-03-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wu, Hao and Noé, Frank},
	year = {2016},
	pages = {4179--4187},
}

@inproceedings{fu_quasi-newton_2016,
	title = {Quasi-{Newton} {Hamiltonian} {Monte} {Carlo}},
	url = {https://pdfs.semanticscholar.org/8922/613da23d2a5c61810aa983e2611e76a8e032.pdf},
	urldate = {2017-03-18},
	booktitle = {Proceedings of the {Thirty}-{Second} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Fu, Tianfan and Luo, Luo and Zhang, Zhihua},
	year = {2016},
	pages = {212--221},
}

@inproceedings{chao_exponential_2015,
	title = {Exponential {Integration} for {Hamiltonian} {Monte} {Carlo}.},
	url = {http://www.jmlr.org/proceedings/papers/v37/chao15.pdf},
	urldate = {2017-03-18},
	booktitle = {{ICML}},
	author = {Chao, Wei-Lun and Solomon, Justin and Michels, Dominik L. and Sha, Fei},
	year = {2015},
	pages = {1142--1151},
}

@inproceedings{chen_stochastic_2014,
	title = {Stochastic {Gradient} {Hamiltonian} {Monte} {Carlo}.},
	url = {http://www.jmlr.org/proceedings/papers/v32/cheni14.pdf},
	urldate = {2017-03-18},
	booktitle = {{ICML}},
	author = {Chen, Tianqi and Fox, Emily B. and Guestrin, Carlos},
	year = {2014},
	pages = {1683--1691},
}

@article{hoffman_no-u-turn_2014,
	title = {The {No}-{U}-turn sampler: adaptively setting path lengths in {Hamiltonian} {Monte} {Carlo}.},
	volume = {15},
	shorttitle = {The {No}-{U}-turn sampler},
	url = {http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf},
	number = {1},
	urldate = {2017-03-18},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D. and Gelman, Andrew},
	year = {2014},
	pages = {1593--1623},
}

@article{bou-rabee_randomized_2015,
	title = {Randomized {Hamiltonian} {Monte} {Carlo}},
	url = {https://arxiv.org/abs/1511.09382},
	urldate = {2017-03-18},
	journal = {arXiv preprint arXiv:1511.09382},
	author = {Bou-Rabee, Nawaf and Sanz-Serna, Jesus Maria},
	year = {2015},
}

@article{hinton_g.e._reducing_2006,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1129198},
	doi = {10.1126/science.1127647},
	language = {en},
	number = {5786},
	urldate = {2017-03-18},
	journal = {Science},
	author = {Hinton, G.E., R.R., Salakhutdinov},
	month = jul,
	year = {2006},
	pages = {504--507},
}

@article{bengio_representation_2013,
	title = {Representation learning: {A} review and new perspectives},
	volume = {35},
	shorttitle = {Representation learning},
	url = {http://ieeexplore.ieee.org/abstract/document/6472238/},
	number = {8},
	urldate = {2017-03-18},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	year = {2013},
	pages = {1798--1828},
}

@book{lee_bayesian_2004,
	title = {Bayesian nonparametrics via neural networks},
	publisher = {SIAM},
	author = {Lee, Herbert KH},
	year = {2004}
}

@inproceedings{bergstra_theano:_2011,
	title = {Theano: {Deep} learning on gpus with python},
	volume = {3},
	shorttitle = {Theano},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.678.1889&rep=rep1&type=pdf},
	urldate = {2017-03-18},
	booktitle = {{NIPS} 2011, {BigLearning} {Workshop}, {Granada}, {Spain}},
	publisher = {Citeseer},
	author = {Bergstra, James and Bastien, Frédéric and Breuleux, Olivier and Lamblin, Pascal and Pascanu, Razvan and Delalleau, Olivier and Desjardins, Guillaume and Warde-Farley, David and Goodfellow, Ian and Bergeron, Arnaud and {others}},
	year = {2011},
}

@inproceedings{abadi_tensorflow:_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {{TensorFlow}},
	url = {https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf},
	urldate = {2017-03-18},
	booktitle = {Proceedings of the 12th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDI}). {Savannah}, {Georgia}, {USA}},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and {others}},
	year = {2016},
}

@inproceedings{jia_caffe:_2014,
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	isbn = {978-1-4503-3063-3},
	shorttitle = {Caffe},
	url = {http://dl.acm.org/citation.cfm?doid=2647868.2654889},
	doi = {10.1145/2647868.2654889},
	language = {en},
	urldate = {2017-03-18},
	publisher = {ACM Press},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	year = {2014},
	pages = {675--678},
}

@article{xie_diversity_2016,
	title = {Diversity {Leads} to {Generalization} in {Neural} {Networks}},
	url = {https://arxiv.org/abs/1611.03131},
	urldate = {2017-03-18},
	journal = {arXiv preprint arXiv:1611.03131},
	author = {Xie, Bo and Liang, Yingyu and Song, Le},
	year = {2016},
}

@article{kawaguchi_deep_2017,
	title = {Deep {Semi}-{Random} {Features} for {Nonlinear} {Function} {Approximation}},
	url = {https://arxiv.org/abs/1702.08882},
	urldate = {2017-03-18},
	journal = {arXiv preprint arXiv:1702.08882},
	author = {Kawaguchi, Kenji and Xie, Bo and Song, Le},
	year = {2017},
}

@inproceedings{kawaguchi_kawaguchi-nips16.pdf_nodate,
	title = {kawaguchi-nips16.pdf},
	author = {Kawaguchi}
}

@inproceedings{kawaguchi_deep_2016,
	title = {Deep {Learning} without {Poor} {Local} {Minima}},
	url = {http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2016, {December} 5-10, 2016, {Barcelona}, {Spain}},
	author = {Kawaguchi, Kenji},
	year = {2016},
	pages = {586--594},
}

@article{schmidhuber_deep_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {08936080},
	shorttitle = {Deep learning in neural networks},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608014002135},
	doi = {10.1016/j.neunet.2014.09.003},
	language = {en},
	urldate = {2017-03-18},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {2015},
	pages = {85--117},
}

@article{horvitz_data_2015,
	title = {Data, privacy, and the greater good},
	volume = {349},
	url = {http://science.sciencemag.org/content/349/6245/253.short},
	number = {6245},
	urldate = {2017-03-18},
	journal = {Science},
	author = {Horvitz, Eric and Mulligan, Deirdre},
	year = {2015},
	pages = {253--255}
}

@article{jordan_machine_2015,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	volume = {349},
	number = {6245},
	journal = {Science},
	author = {Jordan, MI and Mitchell, TM},
	year = {2015},
	pages = {255--260},
}

@article{carpenter_stan:_2016,
	title = {Stan: {A} probabilistic programming language},
	volume = {20},
	shorttitle = {Stan},
	url = {http://www.uvm.edu/~bbeckage/Teaching/DataAnalysis/Manuals/stan-resubmit-JSS1293.pdf},
	urldate = {2017-03-18},
	journal = {Journal of Statistical Software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matt and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Michael A. and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	year = {2016},
}

@article{rinaldi_lattice-boltzmann_2012,
	title = {A {Lattice}-{Boltzmann} solver for 3D fluid simulation on {GPU}},
	volume = {25},
	issn = {1569190X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1569190X1200038X},
	doi = {10.1016/j.simpat.2012.03.004},
	language = {en},
	urldate = {2017-03-19},
	journal = {Simulation Modelling Practice and Theory},
	author = {Rinaldi, P.R. and Dari, E.A. and Vénere, M.J. and Clausse, A.},
	month = jun,
	year = {2012},
	pages = {163--171},
}

@article{obrecht_new_2011,
	title = {A new approach to the lattice {Boltzmann} method for graphics processing units},
	volume = {61},
	issn = {08981221},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S089812211000091X},
	doi = {10.1016/j.camwa.2010.01.054},
	language = {en},
	number = {12},
	urldate = {2017-03-19},
	journal = {Computers \& Mathematics with Applications},
	author = {Obrecht, Christian and Kuznik, Frédéric and Tourancheau, Bernard and Roux, Jean-Jacques},
	month = jun,
	year = {2011},
	pages = {3628--3638},
}

@article{kuznik_lbm_2010,
	title = {{LBM} based flow simulation using {GPU} computing processor},
	volume = {59},
	issn = {08981221},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0898122109006361},
	doi = {10.1016/j.camwa.2009.08.052},
	language = {en},
	number = {7},
	urldate = {2017-03-19},
	journal = {Computers \& Mathematics with Applications},
	author = {Kuznik, Frédéric and Obrecht, Christian and Rusaouen, Gilles and Roux, Jean-Jacques},
	month = apr,
	year = {2010},
	pages = {2380--2392},
}

@article{klingbeil_fat_2012,
	title = {Fat versus {Thin} {Threading} {Approach} on {GPUs}: {Application} to {Stochastic} {Simulation} of {Chemical} {Reactions}},
	volume = {23},
	issn = {1045-9219},
	shorttitle = {Fat versus {Thin} {Threading} {Approach} on {GPUs}},
	url = {http://ieeexplore.ieee.org/document/5871595/},
	doi = {10.1109/TPDS.2011.157},
	number = {2},
	urldate = {2017-03-19},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Klingbeil, Guido and Erban, Radek and Giles, Mike and Maini, Philip K.},
	month = feb,
	year = {2012},
	pages = {280--287},
}

@article{niemeyer_recent_2014,
	title = {Recent progress and challenges in exploiting graphics processors in computational fluid dynamics},
	volume = {67},
	issn = {0920-8542, 1573-0484},
	url = {http://link.springer.com/10.1007/s11227-013-1015-7},
	doi = {10.1007/s11227-013-1015-7},
	language = {en},
	number = {2},
	urldate = {2017-03-19},
	journal = {The Journal of Supercomputing},
	author = {Niemeyer, Kyle E. and Sung, Chih-Jen},
	month = feb,
	year = {2014},
	pages = {528--564},
}

@article{reilly_improved_2005,
	title = {Improved predictions of lynx trappings using a biological model},
	journal = {Applied Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives: An Essential Journey with Donald Rubin's Statistical Family},
	author = {Reilly, Cavan and Zeringue, Angelique},
	year = {2005},
	pages = {297--308},
}

@book{goodfellow_deep_2016,
	series = {Adaptive {Computation} and {Machine} {Learning} {Series}},
	title = {Deep {Learning}},
	isbn = {978-0-262-03561-3},
	url = {https://books.google.com/books?id=Np9SDQAAQBAJ},
	publisher = {MIT Press},
	author = {Goodfellow, I. and Bengio, Y. and Courville, A.},
	year = {2016},
	lccn = {2016022992}
}

@article{wang_identification_2009,
	title = {Identification of {Hammerstein} systems without explicit parameterisation of non-linearity},
	volume = {82},
	number = {5},
	journal = {International Journal of Control},
	author = {Wang, Jiandong and Sano, Akira and Chen, Tongwen and Huang, Biao},
	year = {2009},
	pages = {937--952}
}

@misc{the_mathworks_nonlinear_nodate,
	title = {Nonlinear {Modeling} of a {Magneto}-{Rheological} {Fluid} {Damper}, {Example} file provided by {Matlab} {R}2017a {System} {Identification} {Toolbox}, 2017.},
	url = {https://www.mathworks.com/help/ident/examples/nonlinear-modeling-of-a-magneto-rheological-fluid-damper.html},
	author = {The MathWorks, Inc.}
}

@article{andrieu_particle_2010,
	title = {Particle markov chain monte carlo methods},
	volume = {72},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
	year = {2010},
	pages = {269--342}
}

@article{chopin_smc2:_2013,
	title = {{SMC}2: an efficient algorithm for sequential analysis of state space models},
	volume = {75},
	number = {3},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Chopin, Nicolas and Jacob, Pierre E and Papaspiliopoulos, Omiros},
	year = {2013},
	pages = {397--426}
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	number = {5},
	journal = {Neural networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	year = {1989},
	pages = {359--366}
}

@article{hornik_universal_1990,
	title = {Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
	volume = {3},
	number = {5},
	journal = {Neural networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	year = {1990},
	pages = {551--560}
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	number = {4},
	journal = {Mathematics of Control, Signals, and Systems (MCSS)},
	author = {Cybenko, George},
	year = {1989},
	pages = {303--314}
}

@article{leshno_multilayer_1993,
	title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
	volume = {6},
	number = {6},
	journal = {Neural networks},
	author = {Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
	year = {1993},
	pages = {861--867}
}

@inproceedings{montufar_number_2014,
	title = {On the number of linear regions of deep neural networks},
	booktitle = {Advances in neural information processing systems},
	author = {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
	year = {2014},
	pages = {2924--2932}
}

@inproceedings{ruttor_approximate_2013,
	title = {Approximate {Gaussian} process inference for the drift function in stochastic differential equations},
	urldate = {2017-03-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ruttor, Andreas and Batz, Philipp and Opper, Manfred},
	year = {2013},
	pages = {2040--2048},
}

@article{tran_risk_2014,
	title = {Risk stratification using data from electronic medical records better predicts suicide risks than clinician assessments},
	volume = {14},
	url = {https://bmcpsychiatry.biomedcentral.com/articles/10.1186/1471-244X-14-76?report=reader},
	number = {1},
	urldate = {2017-10-05},
	journal = {BMC psychiatry},
	author = {Tran, Truyen and Luo, Wei and Phung, Dinh and Harvey, Richard and Berk, Michael and Kennedy, Richard Lee and Venkatesh, Svetha},
	year = {2014},
	pages = {76},
}

@article{franklin_risk_2017,
	title = {Risk factors for suicidal thoughts and behaviors: {A} meta-analysis of 50 years of research.},
	volume = {143},
	issn = {1939-1455, 0033-2909},
	shorttitle = {Risk factors for suicidal thoughts and behaviors},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000084},
	doi = {10.1037/bul0000084},
	language = {en},
	number = {2},
	urldate = {2017-10-05},
	journal = {Psychological Bulletin},
	author = {Franklin, Joseph C. and Ribeiro, Jessica D. and Fox, Kathryn R. and Bentley, Kate H. and Kleiman, Evan M. and Huang, Xieyining and Musacchio, Katherine M. and Jaroszewski, Adam C. and Chang, Bernard P. and Nock, Matthew K.},
	year = {2017},
	pages = {187--232},
}

@article{chen_using_2017,
	title = {Using {Hospitalization} and {Mortality} {Data} to {Identify} {Areas} at {Risk} for {Adolescent} {Suicide}},
	volume = {61},
	issn = {1054139X},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1054139X17301106},
	doi = {10.1016/j.jadohealth.2017.02.020},
	language = {en},
	number = {2},
	urldate = {2017-10-05},
	journal = {Journal of Adolescent Health},
	author = {Chen, Kun and Aseltine, Robert H.},
	month = aug,
	year = {2017},
	pages = {192--197},
}

@inproceedings{che_interpretable_2016,
	title = {Interpretable deep models for icu outcome prediction},
	volume = {2016},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333206/},
	urldate = {2017-10-05},
	booktitle = {{AMIA} {Annual} {Symposium} {Proceedings}},
	publisher = {American Medical Informatics Association},
	author = {Che, Zhengping and Purushotham, Sanjay and Khemani, Robinder and Liu, Yan},
	year = {2016},
	pages = {371},
}

@article{delgado-gomez_improving_2011,
	title = {Improving the accuracy of suicide attempter classification},
	volume = {52},
	issn = {09333657},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0933365711000595},
	doi = {10.1016/j.artmed.2011.05.004},
	language = {en},
	number = {3},
	urldate = {2017-10-05},
	journal = {Artificial Intelligence in Medicine},
	author = {Delgado-Gomez, David and Blasco-Fontecilla, Hilario and Alegria, AnaLucia A. and Legido-Gil, Teresa and Artes-Rodriguez, Antonio and Baca-Garcia, Enrique},
	month = jul,
	year = {2011},
	pages = {165--168},
}

@article{delgado-gomez_suicide_2012,
	title = {Suicide attempters classification: {Toward} predictive models of suicidal behavior},
	volume = {92},
	issn = {09252312},
	shorttitle = {Suicide attempters classification},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231212000835},
	doi = {10.1016/j.neucom.2011.08.033},
	language = {en},
	urldate = {2017-10-05},
	journal = {Neurocomputing},
	author = {Delgado-Gomez, David and Blasco-Fontecilla, Hilario and Sukno, Federico and Socorro Ramos-Plasencia, Maria and Baca-Garcia, Enrique},
	month = sep,
	year = {2012},
	pages = {3--8},
}

@inproceedings{haerian_methods_2012,
	title = {Methods for identifying suicide or suicidal ideation in {EHRs}},
	volume = {2012},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3540459/},
	urldate = {2017-10-05},
	booktitle = {{AMIA} {Annual} {Symposium} {Proceedings}},
	publisher = {American Medical Informatics Association},
	author = {Haerian, Krystl and Salmasian, Hojjat and Friedman, Carol},
	year = {2012},
	pages = {1244},
}

@incollection{iliadis_machine_2016,
	address = {Cham},
	title = {Machine {Learning} {Preprocessing} {Method} for {Suicide} {Prediction}},
	volume = {475},
	isbn = {978-3-319-44943-2 978-3-319-44944-9},
	url = {http://link.springer.com/10.1007/978-3-319-44944-9_5},
	urldate = {2017-10-05},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer International Publishing},
	author = {Iliou, Theodoros and Konstantopoulou, Georgia and Ntekouli, Mandani and Lymberopoulos, Dimitrios and Assimakopoulos, Konstantinos and Galiatsatos, Dimitrios and Anastassopoulos, George},
	editor = {Iliadis, Lazaros and Maglogiannis, Ilias},
	year = {2016},
	doi = {10.1007/978-3-319-44944-9_5},
	pages = {53--60},
}

@article{ayat_comparison_2013,
	title = {A comparison of artificial neural networks learning algorithms in predicting tendency for suicide},
	volume = {23},
	issn = {0941-0643, 1433-3058},
	url = {http://link.springer.com/10.1007/s00521-012-1086-z},
	doi = {10.1007/s00521-012-1086-z},
	language = {en},
	number = {5},
	urldate = {2017-10-05},
	journal = {Neural Computing and Applications},
	author = {Ayat, Saeed and Farahani, Hojjat A. and Aghamohamadi, Mehdi and Alian, Mahmood and Aghamohamadi, Somayeh and Kazemi, Zeynab},
	month = oct,
	year = {2013},
	pages = {1381--1386},
}

@inproceedings{ruiz_bayesian_2012,
	title = {Bayesian nonparametric modeling of suicide attempts},
	url = {http://papers.nips.cc/paper/4826-bayesian-nonparametric-modeling-of-suicide-attempts},
	urldate = {2017-10-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Ruiz, Francisco and Valera, Isabel and Blanco, Carlos and Perez-Cruz, Fernando},
	year = {2012},
	pages = {1853--1861},
}

@inproceedings{tran_integrated_2013,
	title = {An integrated framework for suicide risk prediction},
	url = {http://dl.acm.org/citation.cfm?id=2488196},
	urldate = {2017-10-05},
	booktitle = {Proceedings of the 19th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Tran, Truyen and Phung, Dinh and Luo, Wei and Harvey, Richard and Berk, Michael and Venkatesh, Svetha},
	year = {2013},
	pages = {1410--1418},
}

@article{nguyen_evaluation_2016,
	title = {An evaluation of randomized machine learning methods for redundant data: {Predicting} short and medium-term suicide risk from administrative records and risk assessments},
	shorttitle = {An evaluation of randomized machine learning methods for redundant data},
	url = {https://arxiv.org/abs/1605.01116},
	urldate = {2017-10-05},
	journal = {arXiv preprint arXiv:1605.01116},
	author = {Nguyen, Thuong and Tran, Truyen and Gopakumar, Shivapratap and Phung, Dinh and Venkatesh, Svetha},
	year = {2016},
}

@inproceedings{zaher_moderating_2016,
	title = {Moderating the influence of current intention to improve suicide risk prediction},
	volume = {2016},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333240/},
	urldate = {2017-10-05},
	booktitle = {{AMIA} annual symposium proceedings},
	publisher = {American Medical Informatics Association},
	author = {Zaher, Nawal A. and Buckingham, Christopher D.},
	year = {2016},
	pages = {1274},
}

@inproceedings{bhat_bayesian_2016,
	series = {Springer {Proceedings} in {Mathematics} \& {Statistics}},
	title = {Bayesian {Inference} of {Stochastic} {Pursuit} {Models} from {Basketball} {Tracking} {Data}},
	isbn = {978-3-319-54083-2 978-3-319-54084-9},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-54084-9_12},
	doi = {10.1007/978-3-319-54084-9_12},
	abstract = {We develop a Metropolis algorithm to perform Bayesian inference for models given by coupled stochastic differential equations. A key challenge in developing practical algorithms is the computation of the likelihood. We address this problem through the use of a fast method to track the probability density function of the stochastic differential equation. The method applies quadrature to the Chapman–Kolmogorov equation associated with a temporal discretization of the stochastic differential equation. The inference method can be adapted to scenarios in which we have multiple observations at one time, multiple time series, or observations with large and/or irregular temporal spacing. Computational tests show that the resulting Metropolis algorithm is capable of efficient inference for an electrical oscillator model.},
	language = {en},
	urldate = {2017-10-05},
	booktitle = {Bayesian {Statistics} in {Action}},
	publisher = {Springer, Cham},
	author = {Bhat, Harish S. and Madushani, R. W. M. A. and Rawat, Shagun},
	month = jun,
	year = {2016},
	pages = {127--137},
}

@inproceedings{bhat_scalable_2016,
	title = {Scalable {SDE} {Filtering} and {Inference} with {Apache} {Spark}},
	url = {http://proceedings.mlr.press/v53/bhat16.html},
	abstract = {In this paper, we consider the problem of Bayesian filtering and inference for time series data modeled as noisy, discrete-time observations of a stochastic differential equation (SDE) with undeter...},
	language = {en},
	urldate = {2017-10-05},
	booktitle = {{PMLR}},
	author = {Bhat, Harish S. and Madushani, R. W. M. A. and Rawat, Shagun},
	month = dec,
	year = {2016},
	pages = {18--34},
}

@inproceedings{bhat_nonparametric_2016,
	title = {Nonparametric {Adjoint}-{Based} {Inference} for {Stochastic} {Differential} {Equations}},
	doi = {10.1109/DSAA.2016.69},
	abstract = {We develop a nonparametric method to infer the drift and diffusion functions of a stochastic differential equation. With this method, we can build predictive models starting with repeated time series and/or high-dimensional longitudinal data. Typical use of the method includes forecasting the future density or distribution of the variable being measured. The key innovation in our method stems from efficient algorithms to evaluate a likelihood function and its gradient. These algorithms do not rely on sampling, instead, they use repeated quadrature and the adjoint method to enable the inference to scale well as the dimensionality of the parameter vector grows. In simulated data tests, when the number of sample paths is large, the method does an excellent job of inferring drift functions close to the ground truth. We show that even when the method does not infer the drift function correctly, it still yields models with good predictive power. Finally, we apply the method to real data on hourly measurements of ground level ozone, showing that it is capable of reasonable results.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Bhat, H. S. and Madushani, R. W. M. A.},
	month = oct,
	year = {2016},
	keywords = {adjoint method, Biomedical measurement, Computational modeling, differential equations, diffusion function inference, drift function inference, ground level ozone, ground truth, high-dimensional longitudinal data, Mathematical model, nonparametric adjoint-based inference, nonparametric inference, nonparametric statistics, ozone, parameter vector dimensionality, predictive models, Random variables, repeated quadrature, repeated time series, simulated data tests, stochastic differential equations, stochastic processes, Time series analysis},
	pages = {798--807},
}

@inproceedings{bhat_towards_2015,
	title = {Towards scalable quantile regression trees},
	doi = {10.1109/BigData.2015.7363741},
	abstract = {We provide an algorithm to build quantile regression trees in O(N log N) time, where N is the number of instances in the training set. Quantile regression trees are regression trees that model conditional quantiles of the response variable, rather than the conditional expectation as in standard regression trees. We build quantile regression trees by using the quantile loss function in our node splitting criterion. The performance of our algorithm stems from new online update procedures for both the quantile function and the quantile loss function. We test the quantile tree algorithm in three ways, comparing its running time against implementations of standard regression trees, demonstrating its ability to recover a known set of nonlinear quantile functions, and showing that quantile trees yield smaller test set errors (computed using mean absolute deviation) than standard regression trees. The tests include training sets with up to 16 million instances. Overall, our results enable future use of quantile regression trees for large-scale data mining.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Bhat, H. S. and Kumar, N. and Vaz, G. J.},
	month = oct,
	year = {2015},
	keywords = {Approximation methods, computational complexity, data mining, decision trees, large-scale data mining, node splitting criterion, nonlinear quantile functions, online algorithm, online update procedures, Prediction algorithms, quantile loss function, quantile regression, quantile tree algorithm, regression analysis, Regression tree analysis, regression trees, response variable conditional quantiles, scalable quantile regression trees, Standards, Training, Vegetation, Yttrium},
	pages = {53--60},
}

@inproceedings{bhat_citation_2015,
	title = {Citation {Prediction} {Using} {Diverse} {Features}},
	doi = {10.1109/ICDMW.2015.131},
	abstract = {Using a large database of nearly 8 million bibliographic entries spanning over 3 million unique authors, we build predictive models to classify a paper based on its citation count. Our approach involves considering a diverse array of features including the interdisciplinarity of authors, which we quantify using Shannon entropy and Jensen-Shannon divergence. Rather than rely on subject codes, we model the disciplinary preferences of each author by estimating the author's journal distribution. We conduct an exploratory data analysis on the relationship between these interdisciplinarity variables and citation counts. In addition, we model the effects of (1) each author's influence in coauthorship graphs, and (2) words in the title of the paper. We then build classifiers for two-and three-class classification problems that correspond to predicting the interval in which a paper's citation count will lie. We use cross-validation and a true test set to tune model parameters and assess model performance. The best model we build, a classification tree, yields test set accuracies of 0.87 and 0.66, respectively. Using this model, we also provide rankings of attribute importance, for the three-class problem, these rankings indicate the importance of our interdisciplinarity metrics in predicting citation counts.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Data} {Mining} {Workshop} ({ICDMW})},
	author = {Bhat, H. S. and Huang, L. H. and Rodriguez, S. and Dale, R. and Heit, E.},
	month = nov,
	year = {2015},
	keywords = {author journal distribution, bibliographic entries, bibliographic systems, citation analysis, citation count, citation prediction, classification problems, classification tree, coauthorship graphs, data analysis, Databases, data mining, diverse features, entropy, exploratory data analysis, Feature extraction, graph theory, interdisciplinarity, interdisciplinarity variables, Jensen-Shannon divergence, large-scale bibliometrics, Measurement, model parameters, model performance, pattern classification, predictive models, Shannon entropy, test set accuracies, Training, trees (mathematics), unique authors},
	pages = {589--596},
}

@article{bhat_forecasting_2014,
	title = {Forecasting retained earnings of privately held companies with {PCA} and {L}1 regression},
	volume = {30},
	issn = {1526-4025},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/asmb.1972/abstract},
	doi = {10.1002/asmb.1972},
	abstract = {We use proprietary data collected by SVB Analytics, an affiliate of Silicon Valley Bank, to forecast the retained earnings of privately held companies. Combining methods of principal component analysis (PCA) and L1/quantile regression, we build multivariate linear models that feature excellent in-sample fit and strong out-of-sample predictive accuracy. The combined PCA and L1 technique effectively deals with multicollinearity and non-normality of the data, and also performs favorably when compared against a variety of other models. Additionally, we propose a variable ranking procedure that explains which variables from the current quarter are most predictive of the next quarter's retained earnings. We fit models to the top five variables identified by the ranking procedure and thereby, discover interpretable models with excellent out-of-sample performance. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	journal = {Applied Stochastic Models in Business and Industry},
	author = {Bhat, Harish S. and Zaelit, Dan},
	month = may,
	year = {2014},
	keywords = {forecasting, L1 regression, principal component analysis, private companies, quantile regression},
	pages = {271--293},
}

@inproceedings{bhat_fast_2013,
	title = {Fast solution of load shedding problems via a sequence of linear programs},
	doi = {10.1109/BigData.2013.6691770},
	abstract = {Given a power network consisting of nodes (generators/loads) and edges (lines), there exist a set of constraints that must be satisfied in order for the system to be operational. When one or more power lines are cut, the bus phases and load/generator power values may need to be altered in order to restore the system to operation. The load shedding problem is to find the smallest adjustment to the loads that achieves this restoration. In this work, we show how to solve this nonlinear optimization problem by solving a sequence of linear programs. On random graphs with 1500 edges, we find that our method is at least 40 times faster than competing nonlinear optimization methods. We show that our method is capable of solving load shedding problems for a real graph with 19840 edges, and that the method scales with an O(n2) running time where n is the number of edges. For real subgraphs with hundreds of edges, we are able to rapidly solve all possible load shedding problems in which at most two lines are deleted. This work takes a first step towards a scalable load shedding algorithm capable of handling large networks that will arise in the future.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Big} {Data}},
	author = {Bhat, H. S. and Vaz, G. J. and Meza, J. C.},
	month = oct,
	year = {2013},
	keywords = {Equations, Generators, graphs, graph theory, linear programming, linear programs, Load modeling, load shedding, load shedding problems, MATLAB, nonlinear optimization, nonlinear optimization problem, Optimization, power network, random graphs, real subgraphs, scalable load shedding algorithm, sequential linear programming, Vectors},
	pages = {1--6},
}

@inproceedings{bhat_predicting_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Predicting {Private} {Company} {Exits} {Using} {Qualitative} {Data}},
	isbn = {978-3-642-20840-9 978-3-642-20841-6},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-20841-6_33},
	doi = {10.1007/978-3-642-20841-6_33},
	abstract = {Private companies backed by venture capitalists or private equity funds receive their funding in a series of rounds. Information about when each round occurred and which investors participated in each round has been compiled into different databases. Here we mine one such database to model how the private company will exit the VC/PE space. More specifically, we apply a random forest algorithm to each of nine sectors of private companies. Resampling is used to correct imbalanced class distributions. Our results show that a late-stage investor may be able to leverage purely qualitative knowledge of a company’s first three rounds of funding to assess the probability that (1) the company will not go bankrupt and (2) the company will eventually make an exit of some kind (and no longer remain private). For both of these two-class classification problems, our models’ out-of-sample success rate is 75\% and the area under the ROC curve is 0.83, averaged across all sectors. Finally, we use the random forest classifier to rank the covariates based on how predictive they are. The results indicate that the models could provide both predictive and explanatory power for business decisions.},
	language = {en},
	urldate = {2017-10-05},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Bhat, Harish S. and Zaelit, Daniel},
	month = may,
	year = {2011},
	pages = {399--410},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html?foxtrotcallback=true},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2017-10-05},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
}

@article{ma_deep_2015,
	title = {Deep {Neural} {Nets} as a {Method} for {Quantitative} {Structure}–{Activity} {Relationships}},
	volume = {55},
	issn = {1549-9596},
	url = {http://dx.doi.org/10.1021/ci500747n},
	doi = {10.1021/ci500747n},
	abstract = {Neural networks were widely used for quantitative structure–activity relationships (QSAR) in the 1990s. Because of various practical issues (e.g., slow on large problems, difficult to train, prone to overfitting, etc.), they were superseded by more robust methods like support vector machine (SVM) and random forest (RF), which arose in the early 2000s. The last 10 years has witnessed a revival of neural networks in the machine learning community thanks to new methods for preventing overfitting, more efficient training algorithms, and advancements in computer hardware. In particular, deep neural nets (DNNs), i.e. neural nets with more than one hidden layer, have found great successes in many applications, such as computer vision and natural language processing. Here we show that DNNs can routinely make better prospective predictions than RF on a set of large diverse QSAR data sets that are taken from Merck’s drug discovery effort. The number of adjustable parameters needed for DNNs is fairly large, but our results show that it is not necessary to optimize them for individual data sets, and a single set of recommended parameters can achieve better performance than RF for most of the data sets we studied. The usefulness of the parameters is demonstrated on additional data sets not used in the calibration. Although training DNNs is still computationally intensive, using graphical processing units (GPUs) can make this issue manageable.},
	number = {2},
	journal = {Journal of Chemical Information and Modeling},
	author = {Ma, Junshui and Sheridan, Robert P. and Liaw, Andy and Dahl, George E. and Svetnik, Vladimir},
	month = feb,
	year = {2015},
	pages = {263--274},
}

@article{klambauer_self-normalizing_2017,
	title = {Self-{Normalizing} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.02515},
	abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
	journal = {Advances in Neural Information Processing Systems (NIPS)},
	author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02515},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{rotmensch_learning_2017,
	title = {Learning a {Health} {Knowledge} {Graph} from {Electronic} {Medical} {Records}},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-05778-z},
	doi = {10.1038/s41598-017-05778-z},
	abstract = {Demand for clinical decision support systems in medicine and self-diagnostic symptom checkers has substantially increased in recent years. Existing platforms rely on knowledge bases manually compiled through a labor-intensive process or automatically derived using simple pairwise statistics. This study explored an automated process to learn high quality knowledge bases linking diseases and symptoms directly from electronic medical records. Medical concepts were extracted from 273,174 de-identified patient records and maximum likelihood estimation of three probabilistic models was used to automatically construct knowledge graphs: logistic regression, naive Bayes classifier and a Bayesian network using noisy OR gates. A graph of disease-symptom relationships was elicited from the learned parameters and the constructed knowledge graphs were evaluated and validated, with permission, against Google’s manually-constructed knowledge graph and against expert physician opinions. Our study shows that direct and automated construction of high quality health knowledge graphs from medical records using rudimentary concept extraction is feasible. The noisy OR model produces a high quality knowledge graph reaching precision of 0.85 for a recall of 0.6 in the clinical evaluation. Noisy OR significantly outperforms all tested models across evaluation frameworks (p {\textless} 0.01).},
	language = {En},
	number = {1},
	urldate = {2017-10-05},
	journal = {Scientific Reports},
	author = {Rotmensch, Maya and Halpern, Yoni and Tlimat, Abdulhakim and Horng, Steven and Sontag, David},
	month = jul,
	year = {2017},
	pages = {5994},
}

@article{rotmensch_learning_2017-1,
	title = {Learning a {Health} {Knowledge} {Graph} from {Electronic} {Medical} {Records}},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-05778-z},
	doi = {10.1038/s41598-017-05778-z},
	abstract = {Demand for clinical decision support systems in medicine and self-diagnostic symptom checkers has substantially increased in recent years. Existing platforms rely on knowledge bases manually compiled through a labor-intensive process or automatically derived using simple pairwise statistics. This study explored an automated process to learn high quality knowledge bases linking diseases and symptoms directly from electronic medical records. Medical concepts were extracted from 273,174 de-identified patient records and maximum likelihood estimation of three probabilistic models was used to automatically construct knowledge graphs: logistic regression, naive Bayes classifier and a Bayesian network using noisy OR gates. A graph of disease-symptom relationships was elicited from the learned parameters and the constructed knowledge graphs were evaluated and validated, with permission, against Google’s manually-constructed knowledge graph and against expert physician opinions. Our study shows that direct and automated construction of high quality health knowledge graphs from medical records using rudimentary concept extraction is feasible. The noisy OR model produces a high quality knowledge graph reaching precision of 0.85 for a recall of 0.6 in the clinical evaluation. Noisy OR significantly outperforms all tested models across evaluation frameworks (p {\textless} 0.01).},
	language = {En},
	number = {1},
	urldate = {2017-10-05},
	journal = {Scientific Reports},
	author = {Rotmensch, Maya and Halpern, Yoni and Tlimat, Abdulhakim and Horng, Steven and Sontag, David},
	month = jul,
	year = {2017},
	pages = {5994},
}

@inproceedings{krishnan_structured_2017,
	title = {Structured {Inference} {Networks} for {Nonlinear} {State} {Space} {Models}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14215},
	abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
	language = {en},
	urldate = {2017-10-05},
	booktitle = {Thirty-{First} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
	month = feb,
	year = {2017},
}

@article{srivastava_dropout:_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting.},
	volume = {15},
	shorttitle = {Dropout},
	url = {http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer},
	number = {1},
	urldate = {2017-10-05},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}

@article{wilson_marginal_2017,
	title = {The {Marginal} {Value} of {Adaptive} {Gradient} {Methods} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1705.08292},
	abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
	journal = {arXiv:1705.08292 [cs, stat]},
	author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08292},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{abadi_tensorflow:_2016-1,
	title = {Tensorflow: {Large}-scale machine learning on heterogeneous distributed systems},
	shorttitle = {Tensorflow},
	url = {https://arxiv.org/abs/1603.04467},
	urldate = {2017-10-05},
	journal = {arXiv preprint arXiv:1603.04467},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and {others}},
	year = {2016},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {http://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288}
}

@inproceedings{paass_assessing_1993,
	title = {Assessing and improving neural network predictions by the bootstrap algorithm},
	url = {http://papers.nips.cc/paper/659-assessing-and-improving-neural-network-predictions-by-the-bootstrap-algorithm.pdf},
	urldate = {2017-10-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Paass, Gerhard},
	year = {1993},
	pages = {196--203},
}

@article{che_exploiting_2017,
	title = {Exploiting {Convolutional} {Neural} {Network} for {Risk} {Prediction} with {Medical} {Feature} {Embedding}},
	url = {http://arxiv.org/abs/1701.07474},
	abstract = {The widespread availability of electronic health records (EHRs) promises to usher in the era of personalized medicine. However, the problem of extracting useful clinical representations from longitudinal EHR data remains challenging. In this paper, we explore deep neural network models with learned medical feature embedding to deal with the problems of high dimensionality and temporality. Specifically, we use a multi-layer convolutional neural network (CNN) to parameterize the model and is thus able to capture complex non-linear longitudinal evolution of EHRs. Our model can effectively capture local/short temporal dependency in EHRs, which is beneficial for risk prediction. To account for high dimensionality, we use the embedding medical features in the CNN model which hold the natural medical concepts. Our initial experiments produce promising results and demonstrate the effectiveness of both the medical feature embedding and the proposed convolutional neural network in risk prediction on cohorts of congestive heart failure and diabetes patients compared with several strong baselines.},
	journal = {arXiv:1701.07474 [cs, stat]},
	author = {Che, Zhengping and Cheng, Yu and Sun, Zhaonan and Liu, Yan},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.07474},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@misc{noauthor_wan-icml-13.pdf_nodate,
	title = {wan-icml-13.pdf},
	url = {http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf},
	urldate = {2017-10-05}
}

@inproceedings{wan_regularization_2013,
	title = {Regularization of neural networks using dropconnect},
	url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013_wan13},
	urldate = {2017-10-05},
	booktitle = {Proceedings of the 30th international conference on machine learning ({ICML}-13)},
	author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann L. and Fergus, Rob},
	year = {2013},
	pages = {1058--1066},
}

@incollection{bhat_bayesian_2017,
	address = {Cham},
	title = {Bayesian {Inference} of {Stochastic} {Pursuit} {Models} from {Basketball} {Tracking} {Data}},
	isbn = {978-3-319-54084-9},
	url = {https://doi.org/10.1007/978-3-319-54084-9_12},
	abstract = {We develop a Metropolis algorithm to perform Bayesian inference for models given by coupled stochastic differential equations. A key challenge in developing practical algorithms is the computation of the likelihood. We address this problem through the use of a fast method to track the probability density function of the stochastic differential equation. The method applies quadrature to the Chapman–Kolmogorov equation associated with a temporal discretization of the stochastic differential equation. The inference method can be adapted to scenarios in which we have multiple observations at one time, multiple time series, or observations with large and/or irregular temporal spacing. Computational tests show that the resulting Metropolis algorithm is capable of efficient inference for an electrical oscillator model.},
	booktitle = {Bayesian {Statistics} in {Action}: {BAYSM} 2016, {Florence}, {Italy}, {June} 19-21},
	publisher = {Springer International Publishing},
	author = {Bhat, Harish S. and Madushani, R. W. M. A. and Rawat, Shagun},
	editor = {Argiento, Raffaele and Lanzarone, Ettore and Antoniano Villalobos, Isadora and Mattei, Alessandra},
	year = {2017},
	doi = {10.1007/978-3-319-54084-9_12},
	pages = {127--137}
}

@article{bergmann_interdisciplinarity_2017,
	title = {The {Interdisciplinarity} of {Collaborations} in {Cognitive} {Science}},
	volume = {41},
	number = {5},
	journal = {Cognitive Science},
	author = {Bergmann, T. and Dale, R. and Sattari, N. and Heit, E. and Bhat, H.S.},
	year = {2017},
	pages = {1412--1418}
}

@article{bhat_density_2016,
	title = {Density {Tracking} by {Quadrature} for {Stochastic} {Differential} {Equations}},
	journal = {ArXiv e-prints},
	author = {Bhat, Harish S. and Madushani, R. W. M. A.},
	month = oct,
	year = {2016},
	keywords = {60H35, 62M99, 65C30, Mathematics - Numerical Analysis, Mathematics - Probability, Statistics - Computation}
}

@article{bhat_citation_2016,
	title = {Citation {Prediction} {Using} {Diverse} {Features}},
	journal = {Proceedings - 15th IEEE International Conference on Data Mining Workshop, ICDMW 2015},
	author = {Bhat, H.S. and Huang, L.-H. and Rodriguez, S. and Dale, R. and Heit, E.},
	year = {2016},
	pages = {589--596}
}

@inproceedings{bhat_nonparametric_2016-1,
	title = {Nonparametric {Adjoint}-{Based} {Inference} for {Stochastic} {Differential} {Equations}},
	doi = {10.1109/DSAA.2016.69},
	booktitle = {2016 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Bhat, H. S. and Madushani, R. W. M. A.},
	month = oct,
	year = {2016},
	keywords = {adjoint method, Biomedical measurement, Computational modeling, differential equations, diffusion function inference, drift function inference, ground level ozone, ground truth, high-dimensional longitudinal data, Mathematical model, nonparametric adjoint-based inference, nonparametric inference, nonparametric statistics, ozone, parameter vector dimensionality, predictive models, Random variables, repeated quadrature, repeated time series, simulated data tests, stochastic differential equations, stochastic processes, Time series analysis},
	pages = {798--807}
}

@article{bhat_scalable_2016-1,
	title = {Scalable {SDE} {Filtering} and {Inference} with {Apache} {Spark}},
	volume = {53},
	journal = {Proceedings of Machine Learning Research},
	author = {Bhat, Harish S. and Madushani, R. W. M. A. and Rawat, Shagun},
	year = {2016},
	pages = {18--34}
}

@article{bhat_large-scale_2015,
	title = {Large-{Scale} {Empirical} {Tests} of the {Markov} {Tree} {Model}},
	volume = {3},
	number = {3},
	journal = {International Journal of Financial Studies},
	author = {Bhat, Harish S. and Kumar, Nitesh},
	year = {2015},
	pages = {280--318}
}

@article{bhat_computing_2015,
	title = {Computing the density function for a nonlinear stochastic delay system},
	volume = {28},
	number = {12},
	journal = {IFAC-PapersOnLine},
	author = {Bhat, H.S. and Madushani, R.W.M.A.},
	year = {2015},
	pages = {316--321}
}

@article{bhat_towards_2015-1,
	title = {Towards scalable quantile regression trees},
	journal = {Proceedings - 2015 IEEE International Conference on Big Data, IEEE Big Data 2015},
	author = {Bhat, H.S. and Kumar, N. and Vaz, G.J.},
	year = {2015},
	pages = {53--60}
}

@article{bhat_algorithms_2014,
	title = {Algorithms for linear stochastic delay differential equations},
	volume = {114},
	journal = {Springer Proceedings in Mathematics and Statistics},
	author = {Bhat, H.S.},
	year = {2014},
	pages = {57--65}
}

@article{bhat_forecasting_2014-1,
	title = {Forecasting retained earnings of privately held companies with {PCA} and \${L}{\textasciicircum}1\$ regression},
	volume = {30},
	number = {3},
	journal = {Applied Stochastic Models in Business and Industry},
	author = {Bhat, H.S. and Zaelit, D.},
	year = {2014},
	pages = {271--293}
}

@article{becich_cmcpy:_2013,
	title = {{CMCpy}: {Genetic} code-message coevolution models in {Python}},
	volume = {2013},
	number = {9},
	journal = {Evolutionary Bioinformatics},
	author = {Becich, P.J. and Stark, B.P. and Bhat, H.S. and Ardell, D.H.},
	year = {2013},
	pages = {111--125}
}

@article{bhat_fast_2013-1,
	title = {Fast solution of load shedding problems via a sequence of linear programs},
	journal = {Proceedings - 2013 IEEE International Conference on Big Data, Big Data 2013},
	author = {Bhat, H.S. and Vaz, G.J. and Meza, J.C.},
	year = {2013},
	pages = {1--6}
}

@article{bhat_frequency_2013,
	title = {Frequency response and gap tuning for nonlinear electrical oscillator networks},
	volume = {8},
	number = {11},
	journal = {PLoS ONE},
	author = {Bhat, H.S. and Vaz, G.J.},
	year = {2013}
}

@article{bhat_option_2012,
	title = {Option pricing under a normal mixture distribution derived from the {Markov} tree model},
	volume = {223},
	number = {3},
	journal = {European Journal of Operational Research},
	author = {Bhat, H.S. and Kumar, N.},
	year = {2012},
	pages = {762--774}
}

@article{bhat_spectral_2012,
	title = {Spectral solution of delayed random walks},
	volume = {86},
	number = {4},
	journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
	author = {Bhat, H.S. and Kumar, N.},
	year = {2012}
}

@article{bhat_2-d_2011,
	title = {2-{D} inductor-capacitor lattice synthesis},
	volume = {30},
	number = {10},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Bhat, H.S. and Osting, B.},
	year = {2011},
	pages = {1483--1492}
}

@article{bhat_kirchhoffs_2011,
	title = {Kirchhoff's laws as a finite volume method for the planar {Maxwell} equations},
	volume = {59},
	number = {10},
	journal = {IEEE Transactions on Antennas and Propagation},
	author = {Bhat, H.S. and Osting, B.},
	year = {2011},
	pages = {3772--3779}
}

@incollection{bhat_predicting_2011-1,
	address = {Berlin, Heidelberg},
	title = {Predicting {Private} {Company} {Exits} {Using} {Qualitative} {Data}},
	isbn = {978-3-642-20841-6},
	url = {https://doi.org/10.1007/978-3-642-20841-6_33},
	abstract = {Private companies backed by venture capitalists or private equity funds receive their funding in a series of rounds. Information about when each round occurred and which investors participated in each round has been compiled into different databases. Here we mine one such database to model how the private company will exit the VC/PE space. More specifically, we apply a random forest algorithm to each of nine sectors of private companies. Resampling is used to correct imbalanced class distributions. Our results show that a late-stage investor may be able to leverage purely qualitative knowledge of a company's first three rounds of funding to assess the probability that (1) the company will not go bankrupt and (2) the company will eventually make an exit of some kind (and no longer remain private). For both of these two-class classification problems, our models' out-of-sample success rate is 75\% and the area under the ROC curve is 0.83, averaged across all sectors. Finally, we use the random forest classifier to rank the covariates based on how predictive they are. The results indicate that the models could provide both predictive and explanatory power for business decisions.},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}: 15th {Pacific}-{Asia} {Conference}, {PAKDD} 2011, {Shenzhen}, {China}, {May} 24-27, 2011, {Proceedings}, {Part} {I}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bhat, Harish S. and Zaelit, Daniel},
	editor = {Huang, Joshua Zhexue and Cao, Longbing and Srivastava, Jaideep},
	year = {2011},
	doi = {10.1007/978-3-642-20841-6_33},
	pages = {399--410}
}

@article{bhat_discrete_2010,
	title = {Discrete diffraction in two-dimensional transmission line metamaterials},
	volume = {52},
	number = {3},
	journal = {Microwave and Optical Technology Letters},
	author = {Bhat, H.S. and Osting, B.},
	year = {2010},
	pages = {721--725}
}

@article{lilis_harmonic_2010,
	title = {Harmonic generation using nonlinear {LC} lattices},
	volume = {58},
	number = {7 PART 1},
	journal = {IEEE Transactions on Microwave Theory and Techniques},
	author = {Lilis, G.N. and Park, J. and Lee, W. and Li, G. and Bhat, H.S. and Afshari, E.},
	year = {2010},
	pages = {1713--1723}
}

@inproceedings{bhat_markov_2010,
	title = {Markov tree options pricing},
	booktitle = {Proceedings of the 2009 {SIAM} {Conference} on {Mathematics} for {Industry}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Bhat, Harish S. and Kumar, Nitesh},
	year = {2010},
	pages = {162--173}
}

@article{bhat_steady-state_2010,
	title = {Steady-state perturbative theory for nonlinear circuits},
	volume = {43},
	number = {20},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Bhat, H.S. and Lee, W. and Lilis, G.N. and Afshari, E.},
	year = {2010}
}

@article{bhat_diffraction_2009,
	title = {Diffraction on the two-dimensional square lattice},
	volume = {70},
	number = {5},
	journal = {SIAM Journal on Applied Mathematics},
	author = {Bhat, H.S. and Osting, B.},
	year = {2009},
	pages = {1389--1406}
}

@article{bhat_regularization_2009,
	title = {On a regularization of the compressible {Euler} equations for an isothermal gas},
	volume = {358},
	number = {1},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Bhat, H.S. and Fetecau, R.C.},
	year = {2009},
	pages = {168--181}
}

@article{bhat_riemann_2009,
	title = {The {Riemann} problem for the {Leray}-{Burgers} equation},
	volume = {246},
	number = {10},
	journal = {Journal of Differential Equations},
	author = {Bhat, H.S. and Fetecau, R.C.},
	year = {2009},
	pages = {3957--3979}
}

@article{bhat_zone_2009,
	title = {The zone boundary mode in periodic nonlinear electrical lattices},
	volume = {238},
	number = {14},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Bhat, H.S. and Osting, B.},
	year = {2009},
	pages = {1216--1228}
}

@article{bhat_nonlinear_2008,
	title = {Nonlinear constructive interference in electrical lattices},
	volume = {77},
	number = {6},
	journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
	author = {Bhat, H.S. and Afshari, E.},
	year = {2008}
}

@article{bhat_stability_2008,
	title = {Stability of fronts for a regularization of the {Burgers} equation},
	volume = {66},
	number = {3},
	journal = {Quarterly of Applied Mathematics},
	author = {Bhat, H.S. and Fetecau, R.C.},
	year = {2008},
	pages = {473--496}
}

@article{bhat_thin_2008,
	title = {Thin slit diffraction in conventional and dual composite right/left-handed transmission line metamaterials},
	journal = {Proceedings of 2008 Asia Pacific Microwave Conference, APMC 2008},
	author = {Bhat, H.S. and Osting, B.},
	year = {2008}
}

@article{afshari_ultrafast_2008,
	title = {Ultrafast analog {Fourier} transform using 2-{D} {LC} lattice},
	volume = {55},
	number = {8},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Afshari, E. and Bhat, H.S. and Hajimiri, A.},
	year = {2008},
	pages = {2332--2343}
}

@article{bhat_hamiltonian_2006,
	title = {A {Hamiltonian} regularization of the {Burgers} equation},
	volume = {16},
	number = {6},
	journal = {Journal of Nonlinear Science},
	author = {Bhat, H.S. and Fetecau, R.C.},
	year = {2006},
	pages = {615--638}
}

@article{afshari_electrical_2006,
	title = {Electrical funnel: {A} broadband signal combining method},
	journal = {Digest of Technical Papers - IEEE International Solid-State Circuits Conference},
	author = {Afshari, E. and Bhat, H. and Li, X. and Hajimiri, A.},
	year = {2006}
}

@article{afshari_extremely_2006,
	title = {Extremely wideband signal shaping using one- and two-dimensional nonuniform nonlinear transmission lines},
	volume = {99},
	number = {5},
	journal = {Journal of Applied Physics},
	author = {Afshari, E. and Bhat, H.S. and Hajimiri, A. and Marsden, J.E.},
	year = {2006}
}

@article{bhat_lagrangian_2006,
	title = {Lagrangian averaging for the 1D compressible euler equations},
	volume = {6},
	number = {5},
	journal = {Discrete and Continuous Dynamical Systems - Series B},
	author = {Bhat, H.S. and Fetecau, R.C.},
	year = {2006},
	pages = {979--1000}
}

@article{bhat_lagrangian_2005,
	title = {Lagrangian averaging for compressible fluids},
	volume = {3},
	number = {4},
	journal = {Multiscale Modeling and Simulation},
	author = {Bhat, H.S. and Fetecau, R.C. and Marsden, J.E. and Mohseni, K. and West, M.},
	year = {2005},
	pages = {818--837}
}

@article{cybenko_approximation_1989-1,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	number = {4},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314}
}

@inproceedings{klambauer_self-normalizing_2017-1,
	title = {Self-{Normalizing} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.02515},
	abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
	booktitle = {{arXiv}:1706.02515 [cs, stat]},
	author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02515},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{pittsenbarger_trends_2014,
	title = {Trends in {Pediatric} {Visits} to the {Emergency} {Department} for {Psychiatric} {Illnesses}},
	volume = {21},
	issn = {1553-2712},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/acem.12282/abstract},
	doi = {10.1111/acem.12282},
	language = {en},
	number = {1},
	journal = {Academic Emergency Medicine},
	author = {Pittsenbarger, Zachary E. and Mannix, Rebekah},
	month = jan,
	year = {2014},
	pages = {25--30},
}

@misc{noauthor_predicting_nodate,
	title = {Predicting {Risk} of {Suicide} {Attempts} {Over} {Time} {Through} {Machine} {LearningClinical} {Psychological} {Science} - {Colin} {G}. {Walsh}, {Jessica} {D}. {Ribeiro}, {Joseph} {C}. {Franklin}, 2017},
	url = {http://journals.sagepub.com/doi/abs/10.1177/2167702617691560},
	urldate = {2017-10-31}
}

@article{walsh_predicting_2017,
	title = {Predicting risk of suicide attempts over time through machine learning},
	volume = {5},
	number = {3},
	journal = {Clinical Psychological Science},
	author = {Walsh, Colin G. and Ribeiro, Jessica D. and Franklin, Joseph C.},
	year = {2017},
	pages = {457--469},
}

@article{franklin_risk_2017-1,
	title = {Risk factors for suicidal thoughts and behaviors: {A} meta-analysis of 50 years of research.},
	volume = {143},
	issn = {1939-1455, 0033-2909},
	shorttitle = {Risk factors for suicidal thoughts and behaviors},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000084},
	doi = {10.1037/bul0000084},
	language = {en},
	number = {2},
	urldate = {2017-10-31},
	journal = {Psychological Bulletin},
	author = {Franklin, Joseph C. and Ribeiro, Jessica D. and Fox, Kathryn R. and Bentley, Kate H. and Kleiman, Evan M. and Huang, Xieyining and Musacchio, Katherine M. and Jaroszewski, Adam C. and Chang, Bernard P. and Nock, Matthew K.},
	year = {2017},
	pages = {187--232},
}

@inproceedings{kurth_deep_2017,
	title = {Deep learning at 15PF: supervised and semi-supervised classification for scientific data.},
	url = {http://doi.acm.org/10.1145/3126908.3126916},
	doi = {10.1145/3126908.3126916},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}, {SC} 2017, {Denver}, {CO}, {USA}, {November} 12 - 17, 2017},
	author = {Kurth, Thorsten and Zhang, Jian and Satish, Nadathur and Racah, Evan and Mitliagkas, Ioannis and Patwary, Md. Mostofa Ali and Malas, Tareq M. and Sundaram, Narayanan and Bhimji, Wahid and Smorkalov, Mikhail and Deslippe, Jack and Shiryaev, Mikhail and Sridharan, Srinivas and {Prabhat} and Dubey, Pradeep},
	year = {2017},
	pages = {7:1--7:11}
}

@inproceedings{archambeau_variational_2008,
	title = {Variational inference for diffusion processes},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Archambeau, Cédric and Opper, Manfred and Shen, Yuan and Cornford, Dan and Shawe-{T}aylor, John S.},
	year = {2008},
	pages = {17--24},
}

@misc{noauthor_machine_nodate,
	title = {Machine learning of linear differential equations using {Gaussian} processes},
	url = {https://reader.elsevier.com/reader/sd/8CCCB00FE9F36D7421232EC465D820D1F829B88F2670434A4ED531C73850F5FCB78FEA576540AD754B19244670871D24},
	urldate = {2018-05-15}
}

@article{raissi_hidden_2018,
	title = {Hidden physics models: {Machine} learning of nonlinear partial differential equations},
	volume = {357},
	shorttitle = {Hidden physics models},
	doi = {10.1016/j.jcp.2017.11.039},
	abstract = {While there is currently a lot of enthusiasm about “big data”, useful data is usually “small” and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from small data. In particular, we introduce hidden physics models, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier–Stokes, Schrödinger, Kuramoto–Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.},
	journal = {Journal of Computational Physics},
	author = {Raissi, Maziar and Karniadakis, George Em},
	month = mar,
	year = {2018},
	keywords = {Bayesian modeling, Fractional equations, Probabilistic machine learning, Small data, System identification, Uncertainty quantification},
	pages = {125--141},
}

@article{raissi_machine_2017,
	title = {Machine learning of linear differential equations using {Gaussian} processes},
	volume = {348},
	doi = {10.1016/j.jcp.2017.07.050},
	abstract = {This work leverages recent advances in probabilistic machine learning to discover governing equations expressed by parametric linear operators. Such equations involve, but are not limited to, ordinary and partial differential, integro-differential, and fractional order operators. Here, Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations. Such observations may come from experiments or “black-box” computer simulations, as demonstrated in several synthetic examples and a realistic application in functional genomics.},
	journal = {Journal of Computational Physics},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	keywords = {Fractional differential equations, Functional genomics, Inverse problems, Probabilistic machine learning, Uncertainty quantification},
	pages = {683--693},
}

@article{tran_exact_2017,
	title = {Exact {Recovery} of {Chaotic} {Systems} from {Highly} {Corrupted} {Data}},
	volume = {15},
	doi = {10.1137/16M1086637},
	abstract = {Learning the governing equations in dynamical systems from time-varying measurements is of great interest across different scientific fields. This task becomes prohibitive when such data is, moreover, highly corrupted, for example, due to the recording mechanism failing over unknown intervals of time. When the underlying system exhibits chaotic behavior, such as sensitivity to initial conditions, it is crucial to recover the governing equations with high precision. In this work, we consider continuous time dynamical systems \${\textbackslash}dot\{x\} = f(x)\$ where each component of \$f: {\textbackslash}mathbb\{R\}{\textasciicircum}\{d\} {\textbackslash}rightarrow {\textbackslash}mathbb\{R\}{\textasciicircum}d\$ is a multivariate polynomial of maximal degree \$p\$; we aim to identify \$f\$ exactly from possibly highly corrupted measurements \$x(t\_1), x(t\_2), {\textbackslash}dots, x(t\_m)\$. As our main theoretical result, we show that if the system is sufficiently ergodic that this data satisfies a strong central limit theorem (as is known to hold for chaotic Lorenz systems), then the governing equations \$f\$ can be exactly recovered as the solution to an \${\textbackslash}ell\_1\$ minimization problem---even if a large percentage of the data is corrupted by outliers. Numerically, we apply the alternating minimization method to solve the corresponding constrained optimization problem. Through several examples of three-dimensional chaotic systems and higher-dimensional hyperchaotic systems, we illustrate the power, generality, and efficiency of the algorithm for recovering governing equations from noisy and highly corrupted measurement data.},
	number = {3},
	journal = {Multiscale Modeling \& Simulation},
	author = {Tran, G. and Ward, R.},
	month = jan,
	year = {2017},
	pages = {1108--1129},
}

@article{schaeffer_extracting_2017,
	title = {Extracting {Sparse} {High}-{Dimensional} {Dynamics} from {Limited} {Data}},
	abstract = {Extracting governing equations from dynamic data is an essential task in model selection and parameter estimation. The form of the governing equation is rarely known a priori; however, based on the sparsity-of-effect principle one may assume that the number of candidate functions needed to represent the dynamics is very small. In this work, we leverage the sparse structure of the governing equations along with recent results from random sampling theory to develop methods for selecting dynamical systems from under-sampled data. In particular, we detail three sampling strategies that lead to the exact recovery of first-order dynamical systems when we are given fewer samples than unknowns. The first method makes no assumptions on the behavior of the data, and requires a certain number of random initial samples. The second method utilizes the structure of the governing equation to limit the number of random initializations needed. The third method leverages chaotic behavior in the data to construct a nearly deterministic sampling strategy. Using results from compressive sensing, we show that the strategies lead to exact recovery, which is stable to the sparse structure of the governing equations and robust to noise in the estimation of the velocity. Computational results validate each of the sampling strategies and highlight potential applications.},
	journal = {arXiv:1707.08528 [math]},
	author = {Schaeffer, Hayden and Tran, Giang and Ward, Rachel},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.08528},
	keywords = {34F05, 37H99, 65P99, 65L09, 65L99, 37N30, Mathematics - Optimization and Control},
}

@article{schaeffer_learning_2017-1,
	title = {Learning {Dynamical} {Systems} and {Bifurcation} via {Group} {Sparsity}},
	url = {http://arxiv.org/abs/1709.01558},
	abstract = {Learning governing equations from a family of data sets which share the same physical laws but differ in bifurcation parameters is challenging. This is due, in part, to the wide range of phenomena that could be represented in the data sets as well as the range of parameter values. On the other hand, it is common to assume only a small number of candidate functions contribute to the observed dynamics. Based on these observations, we propose a group-sparse penalized method for model selection and parameter estimation for such data. We also provide convergence guarantees for our proposed numerical scheme. Various numerical experiments including the 1D logistic equation, the 3D Lorenz sampled from different bifurcation regions, and a switching system provide numerical validation for our method and suggest potential applications to applied dynamical systems.},
	author = {Schaeffer, Hayden and Tran, Giang and Ward, Rachel},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01558},
	keywords = {65P30, 65K10, 37N30, 15A12, 65L09, 65L99, Mathematics - Numerical Analysis},
}

@article{chen_network_2017,
	title = {Network {Reconstruction} {From} {High}-{Dimensional} {Ordinary} {Differential} {Equations}},
	volume = {112},
	doi = {10.1080/01621459.2016.1229197},
	abstract = {We consider the task of learning a dynamical system from high-dimensional time-course data. For instance, we might wish to estimate a gene regulatory network from gene expression data measured at discrete time points. We model the dynamical system nonparametrically as a system of additive ordinary differential equations. Most existing methods for parameter estimation in ordinary differential equations estimate the derivatives from noisy observations. This is known to be challenging and inefficient. We propose a novel approach that does not involve derivative estimation. We show that the proposed method can consistently recover the true network structure even in high dimensions, and we demonstrate empirical improvement over competing approaches. Supplementary materials for this article are available online.},
	number = {520},
	journal = {Journal of the American Statistical Association},
	author = {Chen, Shizhe and Shojaie, Ali and Witten, Daniela M.},
	month = oct,
	year = {2017},
	keywords = {Additive model, Group lasso, High dimensionality, Ordinary differential equation, Variable selection consistency},
	pages = {1697--1707},
}

@article{quade_sparse_2018,
	title = {Sparse {Identification} of {Nonlinear} {Dynamics} for {Rapid} {Model} {Recovery}},
	url = {http://arxiv.org/abs/1803.00894},
	abstract = {Big data has become a critically enabling component of emerging mathematical methods aimed at the automated discovery of dynamical systems, where first principles modeling may be intractable. However, in many engineering systems, abrupt changes must be rapidly characterized based on limited, incomplete, and noisy data. Many leading automated learning techniques rely on unrealistically large data sets and it is unclear how to leverage prior knowledge effectively to re-identify a model after an abrupt change. In this work, we propose a conceptual framework to recover parsimonious models of a system in response to abrupt changes in the low-data limit. First, the abrupt change is detected by comparing the estimated Lyapunov time of the data with the model prediction. Next, we apply the sparse identification of nonlinear dynamics (SINDy) regression to update a previously identified model with the fewest changes, either by addition, deletion, or modification of existing model terms. We demonstrate this sparse model recovery on several examples for abrupt system change detection in periodic and chaotic dynamical systems. Our examples show that sparse updates to a previously identified model perform better with less data, have lower runtime complexity, and are less sensitive to noise than identifying an entirely new model. The proposed abrupt-SINDy architecture provides a new paradigm for the rapid and efficient recovery of a system model after abrupt changes.},
	author = {Quade, Markus and Abel, Markus and Kutz, J. Nathan and Brunton, Steven L.},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.00894},
	keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Data Analysis, Statistics and Probability},
}

@article{rudy_data-driven_2017,
	title = {Data-driven discovery of partial differential equations},
	volume = {3},
	copyright = {Copyright © 2017, The Authors. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
	doi = {10.1126/sciadv.1602614},
	abstract = {We propose a sparse regression method capable of discovering the governing partial differential equation(s) of a given system by time series measurements in the spatial domain. The regression framework relies on sparsity-promoting techniques to select the nonlinear and partial derivative terms of the governing equations that most accurately represent the data, bypassing a combinatorially large search through all possible candidate models. The method balances model complexity and regression accuracy by selecting a parsimonious model via Pareto analysis. Time series measurements can be made in an Eulerian framework, where the sensors are fixed spatially, or in a Lagrangian framework, where the sensors move with the dynamics. The method is computationally efficient, robust, and demonstrated to work on a variety of canonical problems spanning a number of scientific domains including Navier-Stokes, the quantum harmonic oscillator, and the diffusion equation. Moreover, the method is capable of disambiguating between potentially nonunique dynamical terms by using multiple time series taken with different initial data. Thus, for a traveling wave, the method can distinguish between a linear wave equation and the Korteweg–de Vries equation, for instance. The method provides a promising new technique for discovering governing equations and physical laws in parameterized spatiotemporal systems, where first-principles derivations are intractable.
Researchers propose sparse regression for identifying governing partial differential equations for spatiotemporal systems.
Researchers propose sparse regression for identifying governing partial differential equations for spatiotemporal systems.},
	language = {en},
	number = {4},
	urldate = {2018-05-15},
	journal = {Science Advances},
	author = {Rudy, Samuel H. and Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2017},
	pages = {e1602614},
}

@article{mangan_model_2017,
	title = {Model selection for dynamical systems via sparse regression and information criteria},
	volume = {473},
	copyright = {© 2017 The Authors.. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
	doi = {10.1098/rspa.2017.0009},
	abstract = {We develop an algorithm for model selection which allows for the consideration of a combinatorially large number of candidate models governing a dynamical system. The innovation circumvents a disadvantage of standard model selection which typically limits the number of candidate models considered due to the intractability of computing information criteria. Using a recently developed sparse identification of nonlinear dynamics algorithm, the sub-selection of candidate models near the Pareto frontier allows feasible computation of Akaike information criteria (AIC) or Bayes information criteria scores for the remaining candidate models. The information criteria hierarchically ranks the most informative models, enabling the automatic and principled selection of the model with the strongest support in relation to the time-series data. Specifically, we show that AIC scores place each candidate model in the strong support, weak support or no support category. The method correctly recovers several canonical dynamical systems, including a susceptible-exposed-infectious-recovered disease model, Burgers’ equation and the Lorenz equations, identifying the correct dynamical system as the only candidate model with strong support.},
	language = {en},
	number = {2204},
	urldate = {2018-05-15},
	journal = {Proc. R. Soc. A},
	author = {Mangan, N. M. and Kutz, J. N. and Brunton, S. L. and Proctor, J. L.},
	month = aug,
	year = {2017},
	pages = {20170009},
}

@article{mangan_inferring_2016,
	title = {Inferring {Biological} {Networks} by {Sparse} {Identification} of {Nonlinear} {Dynamics}},
	volume = {2},
	doi = {10.1109/TMBMC.2016.2633265},
	abstract = {Inferring the structure and dynamics of network models is critical to understanding the functionality and control of complex systems, such as metabolic and regulatory biological networks. The increasing quality and quantity of experimental data enable statistical approaches based on information theory for model selection and goodness-of-fit metrics. We propose an alternative data-driven method to infer networked nonlinear dynamical systems by using sparsity-promoting optimization to select a subset of nonlinear interactions representing dynamics on a network. In contrast to standard model selection methods-based upon information content for a finite number of heuristic models (order 10 or less), our model selection procedure discovers a parsimonious model from a combinatorially large set of models, without an exhaustive search. Our particular innovation is appropriate for many biological networks, where the governing dynamical systems have rational function nonlinearities with cross terms, thus requiring an implicit formulation and the equations to be identified in the null-space of a library of mixed nonlinearities, including the state and derivative terms. This method, implicit-SINDy, succeeds in inferring three canonical biological models: 1) Michaelis-Menten enzyme kinetics; 2) the regulatory network for competence in bacteria; and 3) the metabolic network for yeast glycolysis.},
	number = {1},
	journal = {IEEE Transactions on Molecular, Biological and Multi-Scale Communications},
	author = {Mangan, N. M. and Brunton, S. L. and Proctor, J. L. and Kutz, J. N.},
	month = jun,
	year = {2016},
	keywords = {Biochemistry, biological networks, Biological system modeling, Computational modeling, Dynamical systems, Machine learning, network inference, non-convex optimization, Nonlinear dynamical systems, nonlinear dynamics, sparse selection},
	pages = {52--63},
}

@article{meulen_adaptive_2017,
	title = {Adaptive nonparametric drift estimation for diffusion processes using {Faber}-{Schauder} expansions},
	doi = {10.1007/s11203-017-9163-7},
	abstract = {We consider the problem of nonparametric estimation of the drift of a continuously observed one-dimensional diffusion with periodic drift. Motivated by computational considerations, van der Meulen et al. (Comput Stat Data Anal 71:615–632, 2014) defined a prior on the drift as a randomly truncated and randomly scaled Faber–Schauder series expansion with Gaussian coefficients. We study the behaviour of the posterior obtained from this prior from a frequentist asymptotic point of view. If the true data generating drift is smooth, it is proved that the posterior is adaptive with posterior contraction rates for the L\_2L2L\_2-norm that are optimal up to a log factor. Contraction rates in L\_pLpL\_p-norms with p{\textbackslash}in (2,{\textbackslash}infty ]p∈(2,∞]p{\textbackslash}in (2,{\textbackslash}infty ] are derived as well.},
	language = {en},
	urldate = {2018-05-15},
	journal = {Statistical Inference for Stochastic Processes},
	author = {van der Meulen, Frank and Schauer, Moritz and van Waaij, Jan},
	month = jun,
	year = {2017},
	pages = {1--26},
}

@article{van_der_meulen_reversible_2014,
	title = {Reversible jump {MCMC} for nonparametric drift estimation for diffusion processes},
	volume = {71},
	doi = {10.1016/j.csda.2013.03.002},
	abstract = {In the context of nonparametric Bayesian estimation a Markov chain Monte Carlo algorithm is devised and implemented to sample from the posterior distribution of the drift function of a continuously or discretely observed one-dimensional diffusion. The drift is modeled by a scaled linear combination of basis functions with a Gaussian prior on the coefficients. The scaling parameter is equipped with a partially conjugate prior. The number of basis functions in the drift is equipped with a prior distribution as well. For continuous data, a reversible jump Markov chain algorithm enables the exploration of the posterior over models of varying dimension. Subsequently, it is explained how data-augmentation can be used to extend the algorithm to deal with diffusions observed discretely in time. Some examples illustrate that the method can give satisfactory results. In these examples a comparison is made with another existing method as well.},
	journal = {Computational Statistics \& Data Analysis},
	author = {van der Meulen, Frank and Schauer, Moritz and van Zanten, Harry},
	month = mar,
	year = {2014},
	keywords = {Data augmentation, Discretely observed diffusion process, Multiplicative scaling parameter, Nonparametric Bayesian inference, Reversible jump Markov chain Monte Carlo, Series prior},
	pages = {615--632},
}

@article{schon_probabilistic_2018,
	title = {Probabilistic learning of nonlinear dynamical systems using sequential {Monte} {Carlo}},
	volume = {104},
	issn = {0888-3270},
	url = {http://www.sciencedirect.com/science/article/pii/S0888327017305666},
	doi = {10.1016/j.ymssp.2017.10.033},
	abstract = {Probabilistic modeling provides the capability to represent and manipulate uncertainty in data, models, predictions and decisions. We are concerned with the problem of learning probabilistic models of dynamical systems from measured data. Specifically, we consider learning of probabilistic nonlinear state-space models. There is no closed-form solution available for this problem, implying that we are forced to use approximations. In this tutorial we will provide a self-contained introduction to one of the state-of-the-art methods—the particle Metropolis-Hastings algorithm—which has proven to offer a practical approximation. This is a Monte Carlo based method, where the particle filter is used to guide a Markov chain Monte Carlo method through the parameter space. One of the key merits of the particle Metropolis-Hastings algorithm is that it is guaranteed to converge to the “true solution” under mild assumptions, despite being based on a particle filter with only a finite number of particles. We will also provide a motivating numerical example illustrating the method using a modeling language tailored for sequential Monte Carlo methods. The intention of modeling languages of this kind is to open up the power of sophisticated Monte Carlo methods—including particle Metropolis-Hastings—to a large group of users without requiring them to know all the underlying mathematical details.},
	journal = {Mechanical Systems and Signal Processing},
	author = {Schön, Thomas B. and Svensson, Andreas and Murray, Lawrence and Lindsten, Fredrik},
	month = may,
	year = {2018},
	keywords = {Bayesian methods, Metropolis-Hastings, Nonlinear dynamical systems, Parameter estimation, Particle filter, Probabilistic modeling, Sequential Monte Carlo, System identification},
	pages = {866--883},
}

@inproceedings{svensson_computationally_2016,
	title = {Computationally {Efficient} {Bayesian} {Learning} of {Gaussian} {Process} {State} {Space} {Models}},
	url = {http://proceedings.mlr.press/v51/svensson16.html},
	abstract = {Gaussian processes allow for flexible specification of prior assumptions of unknown dynamics in state space models. We present a procedure for efficient Bayesian learning in Gaussian process state ...},
	language = {en},
	urldate = {2018-05-15},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Svensson, Andreas and Solin, Arno and Särkkä, Simo and Schön, Thomas},
	month = may,
	year = {2016},
	pages = {213--221},
}

@book{bhattacharya_stochastic_2009,
	title = {Stochastic {Processes} with {Applications}},
	isbn = {978-0-89871-689-4},
	abstract = {This book develops systematically and rigorously, yet in an expository and lively manner, the evolution of general random processes and their large time properties such as transience, recurrence, and convergence to steady states. The emphasis is on the most important classes of these processes from the viewpoint of theory as well as applications, namely, Markov processes. The book features very broad coverage of the most applicable aspects of stochastic processes, including sufficient material for self-contained courses on random walks in one and multiple dimensions; Markov chains in discrete and continuous times, including birth-death processes; Brownian motion and diffusions; stochastic optimization; and stochastic differential equations. This book is for graduate students in mathematics, statistics, science and engineering, and it may also be used as a reference by professionals in diverse fields whose work involves the application of probability.},
	language = {en},
	publisher = {SIAM},
	author = {Bhattacharya, Rabi N. and Waymire, Edward C.},
	month = aug,
	year = {2009},
	keywords = {Mathematics / Probability \& Statistics / Bayesian Analysis, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{stuart_inverse_2010,
	title = {Inverse problems: {A} {Bayesian} perspective},
	volume = {19},
	shorttitle = {Inverse problems},
	doi = {10.1017/S0962492910000061},
	language = {en},
	urldate = {2018-05-17},
	journal = {Acta Numerica},
	author = {Stuart, A. M.},
	month = may,
	year = {2010},
	pages = {451--559},
}

@book{oksendal_stochastic_2003,
	address = {Berlin Heidelberg},
	edition = {6},
	series = {Universitext},
	title = {Stochastic {Differential} {Equations}: {An} {Introduction} with {Applications}},
	isbn = {978-3-540-04758-2},
	shorttitle = {Stochastic {Differential} {Equations}},
	abstract = {From the reviews of the fifth edition: "This is a highly readable and refreshingly rigorous introduction to stochastic calculus. … This is not a watered-down treatment. It is a serious introduction that starts with fundamental measure-theoretic concepts and ends, coincidentally, with the Black-Scholes formula as one of several examples of applications. This is the best single resource for learning the stochastic calculus … ." (riskbook.com, 2002) From the reviews of the sixth edition: "The book … has evolved from a 200-page typewritten booklet to a modern classic. Part of its charm and success is the fact that the author does not bother too much with the (for the novice) cumbersome rigorous theory … . This does not mean that the book is not rigorous, it is just the timing and dosage of mathematical rigour … that is palatable for undergraduates … . a highly readable account, suitable for self-study and for use in the classroom." (René L. Schilling, The Mathematical Gazette, March, 2005) "This is the sixth edition of the classical and excellent book on stochastic differential equations. The main difference with the next to last edition is the addition of detailed solutions of selected exercises … . This is certainly an excellent idea in view to test its ability of applications of the concepts … . certainly one of the best books on the subject, it will be very helpful to any graduate students and also very valuable for any analysts of financial market." (Stéphane Métens, Physicalia, Vol. 26 (1), 2004) "This is now the sixth edition of the excellent book on stochastic differential equations and related topics. … the presentation is successfully balanced between being easily accessible for a broad audience and being mathematically rigorous. The book is a first choice for courses at graduate level in applied stochastic differential equations. The inclusion of detailed solutions to many of the exercises in this edition also makes it very useful for self-study." (Evelyn Buckwar, Zentralblatt MATH, Vol. 1025, 2003)},
	language = {en},
	urldate = {2018-05-17},
	publisher = {Springer-Verlag},
	author = {Øksendal, Bernt},
	year = {2003},
}

@article{papaspiliopoulos_importance_2012,
	title = {Importance sampling techniques for estimation of diffusion models},
	volume = {124},
	journal = {Statistical methods for stochastic differential equations},
	author = {Papaspiliopoulos, Omiros and Roberts, Gareth~O.},
	year = {2012},
	pages = {311--340},
}

@article{papaspiliopoulos_data_2013,
	title = {Data {Augmentation} for {Diffusions}},
	volume = {22},
	doi = {10.1080/10618600.2013.783484},
	language = {en},
	number = {3},
	urldate = {2018-05-17},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Papaspiliopoulos, Omiros and Roberts, Gareth O. and Stramer, Osnat},
	month = jul,
	year = {2013},
	pages = {665--688},
}

@phdthesis{jensen_statistical_2014,
	type = {{PhD} {Thesis}},
	title = {Statistical inference for partially observed diffusion processes},
	school = {University of Copenhagen, Faculty of Science, Department of Mathematical Sciences},
	author = {Jensen, Anders Christian},
	year = {2014},
}

@article{roberts_inference_2001,
	title = {On inference for partially observed nonlinear diffusion models using the {Metropolis}–{Hastings} algorithm},
	volume = {88},
	doi = {10.1093/biomet/88.3.603},
	abstract = {Abstract.  In this paper, we introduce a new Markov chain Monte Carlo approach to Bayesian analysis of discretely observed diffusion processes. We treat the pat},
	language = {en},
	number = {3},
	urldate = {2018-05-17},
	journal = {Biometrika},
	author = {Roberts, G. O. and Stramer, O.},
	month = oct,
	year = {2001},
	pages = {603--621},
}

@article{ryder_black-box_2018,
	title = {Black-box {Variational} {Inference} for {Stochastic} {Differential} {Equations}},
	journal = {arXiv preprint arXiv:1802.03335},
	author = {Ryder, Thomas and Golightly, Andrew and McGough, A. Stephen and Prangle, Dennis},
	year = {2018},
}

@article{vrettas_variational_2015,
	title = {Variational mean-field algorithm for efficient inference in large systems of stochastic differential equations},
	volume = {91},
	doi = {10.1103/PhysRevE.91.012148},
	abstract = {This work introduces a Gaussian variational mean-field approximation for inference in dynamical systems which can be modeled by ordinary stochastic differential equations. This new approach allows one to express the variational free energy as a functional of the marginal moments of the approximating Gaussian process. A restriction of the moment equations to piecewise polynomial functions, over time, dramatically reduces the complexity of approximate inference for stochastic differential equation models and makes it comparable to that of discrete time hidden Markov models. The algorithm is demonstrated on state and parameter estimation for nonlinear problems with up to 1000 dimensional state vectors and compares the results empirically with various well-known inference methodologies.},
	number = {1},
	journal = {Physical Review E},
	author = {Vrettas, Michail D. and Opper, Manfred and Cornford, Dan},
	month = jan,
	year = {2015},
	pages = {012148},
}

@article{ryder_black-box_2018-1,
	title = {Black-box {Variational} {Inference} for {Stochastic} {Differential} {Equations}},
	journal = {arXiv preprint arXiv:1802.03335},
	author = {Ryder, Thomas and Golightly, Andrew and McGough, A. Stephen and Prangle, Dennis},
	year = {2018},
}

@book{kloeden_numerical_2011,
	title = {Numerical {Solution} of {Stochastic} {Differential} {Equations}},
	isbn = {978-3-540-54062-5},
	abstract = {The aim of this book is to provide an accessible introduction to stochastic differ ential equations and their applications together with a systematic presentation of methods available for their numerical solution. During the past decade there has been an accelerating interest in the de velopment of numerical methods for stochastic differential equations (SDEs). This activity has been as strong in the engineering and physical sciences as it has in mathematics, resulting inevitably in some duplication of effort due to an unfamiliarity with the developments in other disciplines. Much of the reported work has been motivated by the need to solve particular types of problems, for which, even more so than in the deterministic context, specific methods are required. The treatment has often been heuristic and ad hoc in character. Nevertheless, there are underlying principles present in many of the papers, an understanding of which will enable one to develop or apply appropriate numerical schemes for particular problems or classes of problems.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Kloeden, Peter E. and Platen, Eckhard},
	month = jun,
	year = {2011},
	keywords = {Business \& Economics / Statistics, Mathematics / Applied, Mathematics / Calculus, Mathematics / Mathematical Analysis, Mathematics / Number Systems, Mathematics / Numerical Analysis, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Science / Physics / Mathematical \& Computational}
}
