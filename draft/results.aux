\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{ghahramani_learning_1999}
\citation{ruttor_approximate_2013}
\citation{van_der_meulen_reversible_2014,meulen_adaptive_2017}
\citation{brunton_discovering_2016}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{vrettas_variational_2015}
\citation{archambeau_variational_2008}
\citation{batz_variational_2016}
\citation{batz_approximate_2017}
\citation{nicolau_nonparametric_2007,muller_empirical_2010,verzelen_inferring_2012,bhat_nonparametric_2016}
\citation{brunton_discovering_2016,schon_probabilistic_2017,chen_network_2017,tran_exact_2017,schaeffer_extracting_2017,schaeffer_learning_2017,quade_sparse_2018}
\citation{schaeffer_sparse_2013,raissi_machine_2017,rudy_data-driven_2017,raissi_hidden_2018}
\citation{mangan_inferring_2016,mangan_model_2017}
\citation{bhattacharya_stochastic_2009,oksendal_stochastic_2003}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem Setup}{2}{section.2}}
\newlabel{eqnsde}{{1}{2}{Problem Setup}{equation.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Parameterization.}{2}{section*.2}}
\newlabel{eqnhermdef}{{2}{2}{Parameterization}{equation.2.2}{}}
\newlabel{eqnhermmultiindex}{{3}{2}{Parameterization}{equation.2.3}{}}
\newlabel{eqnparam1}{{4}{2}{Parameterization}{equation.2.4}{}}
\citation{roberts_inference_2001,papaspiliopoulos_data_2013}
\citation{papaspiliopoulos_importance_2012}
\citation{stuart_inverse_2010}
\newlabel{eqnparam2}{{5}{3}{Parameterization}{equation.2.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Data.}{3}{section*.3}}
\newlabel{eqneuler}{{6}{3}{Data}{equation.2.6}{}}
\newlabel{eqncondden}{{7}{3}{Data}{equation.2.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Diffusion Bridge.}{3}{section*.4}}
\newlabel{eqnbbridgesde}{{8}{3}{Diffusion Bridge}{equation.2.8}{}}
\newlabel{eqnbbridge}{{9}{3}{Diffusion Bridge}{equation.2.9}{}}
\newlabel{eqnratio}{{10}{3}{Diffusion Bridge}{equation.2.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Expectation Maximization (EM).}{4}{section*.5}}
\newlabel{eqnqfun}{{11}{4}{Expectation Maximization (EM)}{equation.2.11}{}}
\citation{kloeden_numerical_2011}
\newlabel{eqnestM}{{13}{5}{Expectation Maximization (EM)}{equation.2.13}{}}
\newlabel{eqnestrho}{{14}{5}{Expectation Maximization (EM)}{equation.2.14}{}}
\newlabel{eqnestgamma}{{15}{5}{Expectation Maximization (EM)}{equation.2.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{5}{section.3}}
\newlabel{eqn:frob}{{16}{5}{Experiments}{equation.3.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Experiment 1: Varying Number of Time Series.}{6}{section*.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces As we increase the number $S$ of time series used to learn the drift, the estimated drift more closely approximates the ground truth. From top to bottom, left to right, we have plotted estimated and true drifts for the 1D, 2D, and 3D systems. For the 1D and 2D systems, the true drifts depend on only one variable. For the $dX_{1,t}$ component of the 3D system, we have plotted the dependence of the drifts on $X_0$ only, keeping $X_1$ and $X_2$ fixed at $0$.\relax }}{6}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:exp1drift}{{1}{6}{As we increase the number $S$ of time series used to learn the drift, the estimated drift more closely approximates the ground truth. From top to bottom, left to right, we have plotted estimated and true drifts for the 1D, 2D, and 3D systems. For the 1D and 2D systems, the true drifts depend on only one variable. For the $dX_{1,t}$ component of the 3D system, we have plotted the dependence of the drifts on $X_0$ only, keeping $X_1$ and $X_2$ fixed at $0$.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces As we increase the number $S$ of time series used to learn the drift, the Frobenius norm error between estimated and true drifts---see (\ref  {eqn:frob})---decreases significantly. From left to right, we have plotted results for the 1D, 2D, and 3D systems.\relax }}{6}{figure.caption.8}}
\newlabel{fig:exp1hermite}{{2}{6}{As we increase the number $S$ of time series used to learn the drift, the Frobenius norm error between estimated and true drifts---see (\ref {eqn:frob})---decreases significantly. From left to right, we have plotted results for the 1D, 2D, and 3D systems.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Experiment 2: Varying Length of Time Series.}{6}{section*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces We plot true and estimated drifts for the 3D system as a function of increasing time series length $L$. The three components of the vector field are plotted as in the third row of Figure \ref  {fig:exp1drift}. The results show that randomization of observation times compensates for a small value of $L$, enabling accurate estimation.\relax }}{7}{figure.caption.10}}
\newlabel{fig:exp2drift}{{3}{7}{We plot true and estimated drifts for the 3D system as a function of increasing time series length $L$. The three components of the vector field are plotted as in the third row of Figure \ref {fig:exp1drift}. The results show that randomization of observation times compensates for a small value of $L$, enabling accurate estimation.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces As we increase the length $L$ of each time series used for learning, the Frobenius norm error between estimated and true drifts---see (\ref  {eqn:frob})---decreases significantly. From left to right, we have plotted results for the 1D, 2D, and 3D systems.\relax }}{7}{figure.caption.11}}
\newlabel{fig:exp2hermite}{{4}{7}{As we increase the length $L$ of each time series used for learning, the Frobenius norm error between estimated and true drifts---see (\ref {eqn:frob})---decreases significantly. From left to right, we have plotted results for the 1D, 2D, and 3D systems.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Experiment 3: Varying Noise Strength.}{7}{section*.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Varying the strength of the noise in the simulated data alters the quality of estimated drift coefficients, quantified using the Frobenius error (\ref  {eqn:frob}). We proceed from left to right. For the 1D and 2D systems, the maximum noise strength of $0.5$ remains below the magnitude of the drift field coefficients. For these systems, as the noise strength decreases, the error drops close to zero. For the 3D system, the maximum noise strength of $0.5$ is greater than or equal to two of the drift field coefficients, leading to apparently decreased performance---however, see Figure \ref  {fig:exp3drift}.\relax }}{7}{figure.caption.13}}
\newlabel{fig:exp3hermite}{{5}{7}{Varying the strength of the noise in the simulated data alters the quality of estimated drift coefficients, quantified using the Frobenius error (\ref {eqn:frob}). We proceed from left to right. For the 1D and 2D systems, the maximum noise strength of $0.5$ remains below the magnitude of the drift field coefficients. For these systems, as the noise strength decreases, the error drops close to zero. For the 3D system, the maximum noise strength of $0.5$ is greater than or equal to two of the drift field coefficients, leading to apparently decreased performance---however, see Figure \ref {fig:exp3drift}.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Experiment 4: Varying Data Augmentation.}{7}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Though Figure \ref  {fig:exp3hermite} shows a Frobenius norm error for the 3D system greater than $\approx 1.8$ at all noise levels, when plotted, the estimated drift functions lie close to the true drift function. The three components of the vector field are plotted as in the third row of Figure \ref  {fig:exp1drift}.\relax }}{8}{figure.caption.14}}
\newlabel{fig:exp3drift}{{6}{8}{Though Figure \ref {fig:exp3hermite} shows a Frobenius norm error for the 3D system greater than $\approx 1.8$ at all noise levels, when plotted, the estimated drift functions lie close to the true drift function. The three components of the vector field are plotted as in the third row of Figure \ref {fig:exp1drift}.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces As we increase the length $F$ of the diffusion bridge interleaving observed data points, the quality of estimated drifts improves considerably. From left to right, we have plotted Frobenius errors (\ref  {eqn:frob}) between true and estimated coefficients,for the 1D, 2D, and 3D systems.\relax }}{8}{figure.caption.16}}
\newlabel{fig:exp4hermite}{{7}{8}{As we increase the length $F$ of the diffusion bridge interleaving observed data points, the quality of estimated drifts improves considerably. From left to right, we have plotted Frobenius errors (\ref {eqn:frob}) between true and estimated coefficients,for the 1D, 2D, and 3D systems.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Though Figure \ref  {fig:exp4hermite} shows a Frobenius norm error for the 3D system greater than $\approx 2.6$ at all noise levels, when plotted, the estimated drift functions lie close to the true drift function. The three components of the vector field are plotted as in the third row of Figure \ref  {fig:exp1drift}.\relax }}{8}{figure.caption.17}}
\newlabel{fig:exp4drift}{{8}{8}{Though Figure \ref {fig:exp4hermite} shows a Frobenius norm error for the 3D system greater than $\approx 2.6$ at all noise levels, when plotted, the estimated drift functions lie close to the true drift function. The three components of the vector field are plotted as in the third row of Figure \ref {fig:exp1drift}.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{8}{section.4}}
\bibstyle{abbrvnat}
\bibdata{results}
\bibcite{archambeau_variational_2008}{{1}{2008}{{Archambeau et~al.}}{{Archambeau, Opper, Shen, Cornford, and Shawe-{T}aylor}}}
\bibcite{batz_variational_2016}{{2}{2016}{{Batz et~al.}}{{Batz, Ruttor, and Opper}}}
\bibcite{batz_approximate_2017}{{3}{2017}{{Batz et~al.}}{{Batz, Ruttor, and Opper}}}
\bibcite{bhat_nonparametric_2016}{{4}{2016}{{Bhat and Madushani}}{{}}}
\bibcite{bhattacharya_stochastic_2009}{{5}{2009}{{Bhattacharya and Waymire}}{{}}}
\bibcite{brunton_discovering_2016}{{6}{2016}{{Brunton et~al.}}{{Brunton, Proctor, and Kutz}}}
\bibcite{chen_network_2017}{{7}{2017}{{Chen et~al.}}{{Chen, Shojaie, and Witten}}}
\bibcite{ghahramani_learning_1999}{{8}{1999}{{Ghahramani and Roweis}}{{}}}
\bibcite{kloeden_numerical_2011}{{9}{2011}{{Kloeden and Platen}}{{}}}
\bibcite{mangan_inferring_2016}{{10}{2016}{{Mangan et~al.}}{{Mangan, Brunton, Proctor, and Kutz}}}
\bibcite{mangan_model_2017}{{11}{2017}{{Mangan et~al.}}{{Mangan, Kutz, Brunton, and Proctor}}}
\bibcite{muller_empirical_2010}{{12}{2010}{{M\IeC {\"u}ller et~al.}}{{M\IeC {\"u}ller, Yao, and {others}}}}
\bibcite{nicolau_nonparametric_2007}{{13}{2007}{{Nicolau}}{{}}}
\bibcite{papaspiliopoulos_importance_2012}{{14}{2012}{{Papaspiliopoulos and Roberts}}{{}}}
\bibcite{papaspiliopoulos_data_2013}{{15}{2013}{{Papaspiliopoulos et~al.}}{{Papaspiliopoulos, Roberts, and Stramer}}}
\bibcite{quade_sparse_2018}{{16}{2018}{{Quade et~al.}}{{Quade, Abel, Kutz, and Brunton}}}
\bibcite{raissi_hidden_2018}{{17}{2018}{{Raissi and Karniadakis}}{{}}}
\bibcite{raissi_machine_2017}{{18}{2017}{{Raissi et~al.}}{{Raissi, Perdikaris, and Karniadakis}}}
\bibcite{roberts_inference_2001}{{19}{2001}{{Roberts and Stramer}}{{}}}
\bibcite{rudy_data-driven_2017}{{20}{2017}{{Rudy et~al.}}{{Rudy, Brunton, Proctor, and Kutz}}}
\bibcite{ruttor_approximate_2013}{{21}{2013}{{Ruttor et~al.}}{{Ruttor, Batz, and Opper}}}
\bibcite{schaeffer_learning_2017}{{22}{2017}{{Schaeffer}}{{}}}
\bibcite{schaeffer_sparse_2013}{{23}{2013}{{Schaeffer et~al.}}{{Schaeffer, Caflisch, Hauck, and Osher}}}
\bibcite{schaeffer_extracting_2017}{{24}{2017}{{Schaeffer et~al.}}{{Schaeffer, Tran, and Ward}}}
\bibcite{schon_probabilistic_2017}{{25}{2017}{{Sch\IeC {\"o}n et~al.}}{{Sch\IeC {\"o}n, Svensson, Murray, and Lindsten}}}
\bibcite{stuart_inverse_2010}{{26}{2010}{{Stuart}}{{}}}
\bibcite{tran_exact_2017}{{27}{2017}{{Tran and Ward}}{{}}}
\bibcite{van_der_meulen_reversible_2014}{{28}{2014}{{van~der Meulen et~al.}}{{van~der Meulen, Schauer, and van Zanten}}}
\bibcite{meulen_adaptive_2017}{{29}{2017}{{van~der Meulen et~al.}}{{van~der Meulen, Schauer, and van Waaij}}}
\bibcite{verzelen_inferring_2012}{{30}{2012}{{Verzelen et~al.}}{{Verzelen, Tao, M\IeC {\"u}ller, and {others}}}}
\bibcite{vrettas_variational_2015}{{31}{2015}{{Vrettas et~al.}}{{Vrettas, Opper, and Cornford}}}
\bibcite{oksendal_stochastic_2003}{{32}{2003}{{\IeC {\O }ksendal}}{{}}}
