\documentclass{article}

\usepackage[nonatbib]{nips_2018}
\usepackage[numbers,sort&compress]{natbib}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath, amssymb, amsthm}
\usepackage{latexsym, caption, subcaption, verbatim}
\usepackage{graphicx, bm, algorithm, algpseudocode}

\newcommand{\btheta}{\ensuremath{\bm{\theta}}}
\newcommand{\opdiag}{\ensuremath{\operatorname{diag}}}
\newcommand{\bx}{\ensuremath{\mathbf{x}}}
\newcommand{\by}{\ensuremath{\mathbf{y}}}
\newcommand{\bt}{\ensuremath{\mathbf{t}}}
\newcommand{\bz}{\ensuremath{\mathbf{z}}}

\title{Learning Stochastic Dynamical Systems via Bridge Sampling}
\author{
 Harish~S. Bhat\\
 Applied Mathematics Unit\\
 University of California, Merced\\
 Merced, CA 95343\\
 \texttt{hbhat@ucmerced.edu} \\
 \And
 Shagun Rawat\\
 Applied Mathematics Unit\\
 University of California, Merced\\
 Merced, CA 95343\\
 \texttt{srawat2@ucmerced.edu}
}

\begin{document}
\maketitle

\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch
  (3~picas) on both the left- and right-hand margins. Use 10~point
  type, with a vertical spacing (leading) of 11~points.  The word
  \textbf{Abstract} must be centered, bold, and in point size 12. Two
  line spaces precede the abstract. The abstract must be limited to
  one paragraph.
\end{abstract}

The goal of this work is to enable automatic discovery of stochastic differential equations (SDE) from time series data.  

\begin{enumerate}
\item Literature review. 
simple, effective procedure for ODE (not SDE): \cite{brunton_discovering_2016, mangan_inferring_2016, mangan_model_2017}
low-dimensional direct approach with gradient-based optimization (not scalable to higher dim): \cite{bhat_nonparametric_2016}
diffusion bridges but more complex estimation (not EM!): \cite{van_der_meulen_reversible_2014, meulen_adaptive_2017}
classical stat approaches: \cite{nicolau_nonparametric_2007, muller_empirical_2010, verzelen_inferring_2012}
PDE: \cite{schaeffer_sparse_2013, schaeffer_learning_2017, rudy_data-driven_2017}
compressed sensing: \cite{tran_exact_2017, schaeffer_extracting_2017}

\item What is new and interesting about this work.
\end{enumerate}

Points to cover:
\begin{itemize}
\item data specification, DONE
\item Hermite polynomial and drift function representation, DONE
\item Expectation and maximization formulas assuming data is filled in, CLOSE2DONE
\item Filling data in with diffusion bridge, DONE
\item MCMC iterations of brownian bridge using girsanov likelihood, DONE
\item how synthetic data is generated
\item results: 1D, 2D, 3D damped duffing, 3D lorenz
\item plots: error of theta vs noise, error vs amount of data (number of data points) parametric curves for noise levels, brownian bridge plots for illustration, ...
\end{itemize}

\section{Problem Setup}
Let $W_t$ denote Brownian motion in $\mathbb{R}^d$---informally, an increment $dW_t$ of this process has a multivariate normal distribution with zero mean vector and covariance matrix $I dt$.  Let $X_t$ denote an $\mathbb{R}^d$-valued stochastic process that evolves according to the It\^{o} SDE
\begin{equation} \label{eqn:sde}
d X_t = f( X_t) dt + \Gamma d W_t.
\end{equation}
For rigorous definitions of Brownian motion and SDE, see \cite{bhattacharya_stochastic_2009, oksendal_stochastic_2003}.  The nonlinear vector field $f : \Omega \subset \mathbb{R}^d \to \mathbb{R}^d$ is the \emph{drift} function, and the $d \times d$ matrix $\Gamma$ is the \emph{diffusion} matrix.  To reduce the number of model parameters, we assume $\Gamma = \opdiag \gamma$.

\textbf{Our goal is to develop an algorithm that accurately estimates the functional form of $f$ and the vector $\gamma$ from time series data.}

\paragraph{Parameterization.} We parameterize $f$ using Hermite polynomials.  The $n$-th Hermite polynomial takes the form
\begin{equation}
\label{eqn:hermdef}
H_n(x) = (\sqrt{2 \pi} n!)^{-1/2} (-1)^n e^{x^2/2} \dfrac{\mathrm{d}^n}{\mathrm{d}x^n} e^{-x^2/2}
\end{equation}
Let $\langle f, g \rangle_w = \int_{\mathbb{R}} f(x) g(x) \exp(-x^2/2) \, dx$ denote a weighted $L^2$ inner product.  Then, $\langle H_i, H_j \rangle_w = \delta_{ij}$, i.e., the Hermite polynomials are orthonormal with respect to the weighted inner product.  In fact, with respect to this inner product, the Hermite polynomials form an orthonormal basis of $L^2_w(\mathbb{R}) = \{ f \, : \, \langle f, f \rangle_w < \infty \}$.

Now let $\alpha = (\alpha_1, \ldots, \alpha_d) \in \mathbb{Z}^d_+$ denote a multi-index.  We use the notation $|\alpha| = \sum_j \alpha_j$ and $x^\alpha = \prod_j (x_j)^{\alpha_j}$ for $x = (x_1, \ldots, x_d) \in \mathbb{R}^d$.  For $x \in \mathbb{R}^d$ and a multi-index $\alpha$, we also define
\begin{equation}
\label{eqn:hermmultiindex}
H_\alpha(x) = \prod_{j=1}^d H_{\alpha_j}(x_j).
\end{equation}
We write $f(x) = (f_1(x), \ldots f_d(x))$ and then parameterize each component:
\begin{equation}
\label{eqn:param1}
f_j(x) = \sum_{m=0}^M \sum_{|\alpha|=m} \beta^j_\alpha H_\alpha(x).
\end{equation}
We see that the maximum degree of $H_\alpha(x)$ is $|\alpha|$.  Hence we think of the double sum in (\ref{eqn:param1}) as first summing over degrees and then summing over all terms with a fixed maximum degree.  We say maximum degree because, for instance, $H_2(z) = (z^2-1)/(\sqrt{2 \pi} 2)^{1/2}$ contains both degree $2$ and degree $0$ terms.

There are $\binom{m + d - 1}{d-1}$ possibilities for a $d$-dimensional multi-index $\alpha$ such that $|\alpha| = m$.  Summing this from $m=0$ to $M$, there are $\widetilde{M} = \binom{M+d}{d}$ total multi-indices in the double sum in (\ref{eqn:param1}).  Let $(i)$ denote the $i$-th multi-index according to some ordering.  Then we can write
\begin{equation}
\label{eqn:param2}
f_j(x) = \sum_{i=1}^{\widetilde{M}} \beta^j_{(i)} H_{(i)}(x).
\end{equation}

\paragraph{Data.} We consider our data $\bx = \{x_j\}_{j=0}^L$ to be direct observations of $X_t$ at discrete points in time $\bt = \{t_j\}_{t=0}^L$.  Note that these time points do not need to be equispaced.  

To achieve our estimation goal, we apply expectation maximization (EM).  We regard $\bx$ as the incomplete data.  Let $\Delta t = \max_{j} (t_j - t_{j-1})$ be the maximum interobservation spacing.  We think of the missing data $\bz$ as data collected at a time scale $h \ll \Delta t$ that is fine enough such that the transition density of (\ref{eqn:sde}) is approximately Gaussian.  To see how this works, let $\mathcal{N}(\mu, \Sigma)$ denote a multivariate normal with mean vector $\mu$ and covariance matrix $\Sigma$.  Now discretize (\ref{eqn:sde}) in time via the Euler-Maruyama method with time step $h > 0$; the result is
\begin{equation} \label{eqn:euler}
\widetilde{X}_{n+1} = \widetilde{X}_n + f(\widetilde{X}_n) h + h^{1/2} \Gamma Z_{n+1},
\end{equation}
where $Z_{n+1} \sim \mathcal{N}(0, I)$ is a standard multivariate normal, independent of $X_n$.  This implies that
\begin{equation}
\label{eqn:condden}
(\widetilde{X}_{n+1} | \widetilde{X}_n = v) \sim \mathcal{N}(v + f(v) h, h \Gamma^2).
\end{equation}
As $h$ decreases, $\widetilde{X}_{n+1} | \widetilde{X}_n = v$---a Gaussian approximation---will converge to the true transition density $X_{(n+1)h} | X_{nh} = v$, where $X_t$ refers to the solution of (\ref{eqn:sde}).

\paragraph{Diffusion Bridge.} To augment or complete the data, we employ diffusion bridge sampling, using a Markov chain Monte Carlo (MCMC) method that goes back to \cite{roberts_inference_2001, papaspiliopoulos_data_2013}.  Let us describe our version here.  We suppose our current estimate of $\btheta = (\beta, \gamma)$ is given.  Define the diffusion bridge process to be (\ref{eqn:sde}) conditioned on both the initial value $x_i$ at time $t_i$, and the final value $x_{i+1}$ at time $t_{i+1}$.  The goal is to generate sample paths of this diffusion bridge.  By a sample path, we mean $F-1$ \emph{new} samples $\{z_{i,j}\}_{j=1}^{F-1}$ at times $t_i + j h$ with $h = (t_{i+1} - t_i)/F$.

To generate such a path, we start by drawing a sample from a Brownian bridge with the same diffusion as (\ref{eqn:sde}).  That is, we sample from the SDE
\begin{equation}
\label{eqn:bbridgesde}
d\widehat{X}_t = \Gamma dW_t
\end{equation}
conditioned on $\widehat{X}_{t_i} = x_i$ and $\widehat{X}_{t_{i+1}} = x_{i+1}$.  This Brownian bridge can be described explicitly:
\begin{equation}
\label{eqn:bbridge}
\widehat{X}_t = \Gamma (W_{t} - W_{t_i}) + x_i - \frac{t - t_i}{t_{i+1} - t_i} (\Gamma (W_{t_{i+1}} - W_{t_i}) + x_{i} - x_{i+1} )
\end{equation}
Here $W_0 = 0$ (almost surely), and $W_t - W_s \sim \mathcal{N}(0, (t-s)I)$ for $t > s \geq 0$. 

Let $\mathbb{P}$ denote the law of the diffusion bridge process, and let $\mathbb{Q}$ denote the law of the Brownian bridge (\ref{eqn:bbridge}).  Using Girsanov's theorem \cite{papaspiliopoulos_importance_2012}, we can show that
\begin{equation}
\label{eqn:ratio}
\frac{d \mathbb{P}}{d \mathbb{Q}} = C \exp \left( \int_{t_i}^{t_{i+1}} f(\widehat{X}_s)^T \Gamma^{-2} \, d \widehat{X}_s - \frac{1}{2} \int_{t_i}^{t_{i+1}} f(\widehat{X_s})^T \Gamma^{-2} f(\widehat{X_s}) \, ds \right),
\end{equation}
where the constant $C$ depends only on $x_i$ and $x_{i+1}$.  The left-hand side is a Radon-Nikodym derivative, equivalent to a density or likelihood; the ratio of two such likelihoods is the accept/reject ratio in the Metropolis algorithm [Stuart 2010].  

Putting the above pieces together yields the following Metropolis algorithm to generate diffusion bridge sample paths.  Fix $F \geq 2$ and $i \in \{0, \ldots, L-1\}$.  Assume we have stored the previous Metropolis step, i.e., a path $\bz^{(\ell)} = \{z_{i,j}\}_{j=1}^{F-1}$.
\begin{enumerate}
\item Use (\ref{eqn:bbridge}) to generate samples of $\widehat{X}_t$ at times $t_i + j h$, for $j = 1, 2, \ldots, F-1$ and $h = (t_{i+1} - t_i)/F$.  This is the proposal $\bz^\ast = \{z^\ast_{i,j}\}_{j=1}^{F-1}$.
\item Numerically approximate the integrals in (\ref{eqn:ratio}) to compute the likelihood of the proposal.  Specifically, we compute
\begin{multline*}
p(\bz^\ast)/C = \sum_{j=0}^{F-1} f(z^\ast_{i,j})^T \Gamma^{-2} (z^\ast_{i,j+1} - z^\ast_{i,j}) \\ - \frac{h}{4} \sum_{j=0}^{F-1} \left[ f(z^\ast_{i,j})^T \Gamma^{-2} f(z^\ast_{i,j}) + f(z^\ast_{i,j+1})^T \Gamma^{-2} f(z^\ast_{i,j+1}) \right]
\end{multline*}
\item Accept the proposal with probability $p(\bz^\ast)/p(\bz^{(\ell)})$---note the factors of $C$ cancel.  If the proposal is accepted, then set $\bz^{(\ell+1)} = \bz^\ast$. Else set $\bz^{(\ell+1)} = \bz^{(\ell)}$.
\end{enumerate}
We initialize this algorithm with a Brownian bridge path, run for $10$ burn-in steps, and then use subsequent steps as the diffusion bridge samples we seek. 


\paragraph{Expectation Maximization (EM).} Let us now give details to justify the intuition expressed above, that employing the diffusion bridge to augment the data on a fine scale will enable estimation.  Let $\bz$ a diffusion bridge sample path.  We interleave this sampled data together with the observed data $\bx$ to create the completed time series
$$
\by^{(r)} = \{y_j^{(r)}\}_{j=1}^N,
$$
where $N = LF + 1$.  By interleaving, we mean that $y_{1 + i F}^{(r)} = x_i$ for $i = 0, 1, \ldots, L$, and that $y_{1 + j + i F}^{(r)} = z_{i, j}$ for $j = 1, 2, \ldots, F-1$ and $i = 0, 1, \ldots, L-1$.

Let $h_j$ denote the elapsed time between observations $y_j$ and $y_{j+1}$.  Using the completed data, the temporal discretization (\ref{eqn:euler}) of the SDE, the Markov property, and property (\ref{eqn:condden}), we have:
\begin{align*}
\log p(\bx, \bz \mid \btheta) &= \log p(\by \mid \btheta) \\
 &= \sum_{j=1}^{N-1} \log p(y_{j+1} \mid y_j, \btheta) \\
 &= -\sum_{j=1}^{N-1} \Biggl[ \sum_{i=1}^d \frac{1}{2} \log(2 \pi h_j \gamma_i^2) \\
 &+ \frac{1}{2h_j} (y_{j+1} - y_j - h_j \sum_{k=1}^{\widetilde{M}} \beta_{(k)} H_{(k)}(y_j))^T \Gamma^{-2} (y_{j+1} - y_j - h_j \sum_{\ell=1}^{\widetilde{M}} \beta_{(\ell)} H_{(\ell)}(y_j)) \Biggr].
\end{align*}

\paragraph{EM.} The EM algorithm consists of two steps, computing the expectation of the log likelihood function (on the completed data) and then maximizing it with respect to the parameters 
\begin{enumerate}
\item Start with an initial guess for the parameters, $\btheta^{(0)}$.
\item For the expectation (or E) step,
\begin{equation}
\label{eqn:expectation}
Q(\btheta, \btheta^{(k)}) = \mathbb{E}_{\bz \mid \bx, \btheta^{(k)}} [\log p(\bx, \bz \mid \btheta)]
\end{equation}
Our plan is to evaluate this expectation via bridge sampling.  That is, we will sample from diffusion bridges $\bz \mid \bx, \btheta^{(k)}$.  Then $(\bx, \bz)$ will be a combination of the original data together with sample paths.
\item For the maximization (or M) step, we start with the current iterate and a dummy variable $\btheta$ and define
\begin{equation}
\label{eqn:maximization}
\btheta^{(k+1)} = \arg \max_{\btheta} Q(\btheta, \btheta^{(k)})
\end{equation}
It will turn out that we can maximize this quantity without numerical optimization.  All we will need to do is solve a least-squares problem.
\item Iterate Step 2 and 3 until convergence.
\end{enumerate}

\paragraph{Details.} With a fixed parameter vector $\btheta^{(k)}$, the SDE (\ref{eqn:sde}) is specified completely, i.e., the drift and diffusion terms have no further unknowns.  


Suppose we form $R$ such time series.  The expected log likelihood can then be approximated by
\begin{align*}
Q(\btheta, \btheta^{(k)}) &= \mathbb{E}_{\bz \mid \bx, \btheta^{(k)}} [\log p(\bx, \bz \mid \btheta)] \\
 &\approx \frac{1}{R} \sum_{r=1}^R \biggl[ \sum_{j=1}^N \left[ \sum_{i=1}^d -\frac{1}{2} \log (2 \pi h \gamma_i^2) \right] \\
 &\qquad -\frac{1}{2h} (y_j^{(r)} - y_{j-1}^{(r)} - h \sum_{k=1}^M \beta_k \phi_k(y_{j-1}^{(r)}))^T \Gamma^{-2} (y_j^{(r)} - y_{j-1}^{(r)} - h \sum_{\ell=1}^M \beta_\ell \phi_\ell(y_{j-1}^{(r)}) ) \biggr] 
\end{align*}

To maximize $Q$ over $\btheta$, we first assume $\Gamma = \opdiag \gamma$ is known and maximize over $\beta$.  This is a least squares problem.  The solution is given by forming the matrix
$$
\mathcal{M}_{k,\ell} = \frac{1}{R} \sum_{r=1}^{R} \sum_{j=1}^N h \phi_k^T (y_{j-1}^{(r)}) \Gamma^{-2} \phi_\ell^T (y_{j-1}^{(r)})
$$
and the vector
$$
\rho_k = \frac{1}{R} \sum_{r=1}^{R} \sum_{j=1}^N \phi_k^T (y_{j-1}^{(r)}) \Gamma^{-2} (y_j^{(r)} - y_{j-1}^{(r)}).
$$

We then solve the system
$$
\mathcal{M} \beta = \rho
$$
for $\beta$.  Now that we have $\beta$, we maximize $Q$ over $\gamma$.  The solution can be obtained in closed form:
$$
\gamma_i^2 = \frac{1}{R N h} \sum_{r=1}^{R} \sum_{j=1}^N (( y_j^{(r)} - y_{j-1}^{(r)} - h \sum_{\ell=1}^M \beta_\ell \phi_\ell (y_{j-1}^{(r)}) ) \cdot e_i )^2
$$
where $e_i$ is the $i^\text{th}$ canonical basis vector in $\mathbb{R}^d$.

We demonstrate the method for 1, 2 and 3 dimensional systems. 
\begin{itemize}
\item For the 1-dimensional system, we use the ? oscillator:
\begin{equation}
\mathrm{d}X(t) = (\alpha X(t) + \beta X(t)^2 + \gamma) \: \mathrm{d}t + g \: \mathrm{d}W(t)
\end{equation}
\item For the 2-dimensional system, we use the undamped Duffing oscillator:
\begin{align*}
\mathrm{d}X_1(t) & = X_2(t) \mathrm{d}t + g_1 \: \mathrm{d} W_1(t) \\
\mathrm{d}X_2(t) & = (-X_1(t) - X^3_1(t)) \mathrm{d}t + g_2 \: \mathrm{d} W_2(t)
\end{align*}
\item For the 3-dimensional case, we consider 2 different form of equations. The first one is the damped Duffing oscillator, a general form of the damped oscillator considered in the 2-dimensional case:
\begin{align*}
\mathrm{d}X_1(t) & = X_2(t) \: \mathrm{d}t + g_1 \: \mathrm{d}W_1(t) \\
\mathrm{d}X_2(t) & = (\alpha X_1(t) - \beta X_1(t) - \delta X_2(t) + \gamma \cos (X_3(t))) \: \mathrm{d}t + g_2 \: \mathrm{d}W_2(t) \\
\mathrm{d}X_3(t) & = \omega \: \mathrm{d}t + g_3 \: \mathrm{d}W_3(t)
\end{align*}
\item Another example considered for the 3-dimensional case is the Lorenz oscillator:
\begin{align*}
\mathrm{d}X_1(t) & = \sigma (X_2(t) - X_1(t)) \: \mathrm{d}t + g_1 \: \mathrm{d}W_1(t) \\
\mathrm{d}X_2(t) & = (X_1(t) (\rho - X_3(t))) \mathrm{d}t + g_2 \: \mathrm{d}W_2(t) \\
\mathrm{d}X_3(t) & = (X_1(t) X_2(t) - \beta X_3(t)) \: \mathrm{d}t + g_3 \: \mathrm{d}W_3(t)
\end{align*}
\end{itemize}
For simplicity, consider the example where the $X \in \mathbb{R}^2$ and the highest degree of the Hermite polynomial is three, including four Hermite polynomials:
\begin{align*}
f(x_1, x_2) & = \sum_{m = 0}^{2} \sum_{i+j = 0}^{i+j = m} \zeta_{i,j} \: \psi_{i,j} \\
& = \sum_{d = 0}^{3} \sum_{i + j = 0}^{i + j = 3} \zeta_{i, j} H_i(x_1) H_j(x_2) \\
& = \sum_{i + j = 0} \zeta_{i, j} H_i(x_1) H_j(x_2) + \sum_{i + j = 1} \zeta_{i, j} H_i(x_1) H_j(x_2) + \sum_{i + j = 2} \zeta_{i, j} H_i(x_1) H_j(x_2) + \sum_{i + j = 3} \zeta_{i, j} H_i(x_1) H_j(x_2) \\
& = \zeta_{0, 0} H_0(x_1)H_0(x_2) + \zeta_{0, 1}H_0(x_1)H_1(x_2) + \zeta_{1, 0}H_1(x_1)H_0(x_2) + \zeta_{0, 2}H_0(x_1)H_2(x_2) \\ & + \zeta_{2, 0}H_2(x_1)H_0(x_2) + \zeta_{1, 1}H_1(x_1)H_1(x_2) + \zeta_{0, 3}H_0(x_1)H_3(x_2) + \zeta_{3, 0} H_3(x_1)H_0(x_2) \\ & + \zeta_{2, 1} H_2(x_1)H_1(x_2) + \zeta_{1, 2} H_1(x_1)H_2(x_2)
\end{align*}

{\small
\bibliographystyle{abbrvnat}
\bibliography{results}
}

\end{document}
