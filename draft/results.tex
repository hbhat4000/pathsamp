\documentclass{article}

\usepackage{nips_2018}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath, amssymb, amsthm}
\usepackage{latexsym, caption, subcaption, verbatim}
\usepackage{graphicx, bm, algorithm, algpseudocode}

\newcommand{\btheta}{\ensuremath{\bm{\theta}}}
\newcommand{\opdiag}{\ensuremath{\operatorname{diag}}}
\newcommand{\bx}{\ensuremath{\mathbf{x}}}
\newcommand{\bz}{\ensuremath{\mathbf{z}}}

\title{Learning Stochastic Dynamical Systems via Bridge Sampling}
\author{
 Harish~S. Bhat\\
 Applied Mathematics Unit\\
 University of California, Merced\\
 Merced, CA 95343\\
 \texttt{hbhat@ucmerced.edu} \\
 \And
 Shagun Rawat\\
 Applied Mathematics Unit\\
 University of California, Merced\\
 Merced, CA 95343\\
 \texttt{srawat2@ucmerced.edu}
}

\begin{document}
\maketitle

\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch
  (3~picas) on both the left- and right-hand margins. Use 10~point
  type, with a vertical spacing (leading) of 11~points.  The word
  \textbf{Abstract} must be centered, bold, and in point size 12. Two
  line spaces precede the abstract. The abstract must be limited to
  one paragraph.
\end{abstract}

The goal of this work is to enable automatic discovery of stochastic differential equations (SDE) from time series data.  

\begin{enumerate}
\item Literature review.
\item What is new and interesting about this work.
\end{enumerate}

Points to cover:
\begin{itemize}
\item data specification
\item Hermite polynomial and drift function representation
\item Expectation and maximization formulas assuming data is filled in
\item Filling data in with Brownian bridge
\item MCMC iterations of brownian bridge using girsanov likelihood
\item how synthetic data is generated
\item results: 1D, 2D, 3D damped duffing, 3D lorenz
\item plots: error of theta vs noise, error vs amount of data (number of data points) parametric curves for noise levels, brownian bridge plots for illustration, ...
\item Note: constant noise case, not inferring the gvec
\end{itemize}

\section{Problem Setup}
Let $W_t$ denote Brownian motion in $\mathbb{R}^d$---informally, an increment $dW_t$ of this process has a multivariate normal distribution with zero mean vector and covariance matrix $I dt$.  Let $X_t$ denote an $\mathbb{R}^d$-valued stochastic process that evolves according to the It\^{o} SDE
\begin{equation} \label{eqn:sde}
d X_t = f( X_t) dt + \Gamma d W_t.
\end{equation}
For rigorous definitions of Brownian motion and SDE, see [Bhattacharya and Waymire 2009, Oksendal 2007].
The nonlinear vector field $f : \Omega \subset \mathbb{R}^d \to \mathbb{R}^d$ is the \emph{drift} function, and the $d \times d$ matrix $\Gamma$ is the \emph{diffusion} matrix.  To reduce the number of model parameters, we assume $\Gamma = \opdiag \gamma$.

\textbf{Our goal is to develop an algorithm that accurately estimates the functional form of $f$ and the vector $\gamma$ from time series data.}

\paragraph{Parameterization.} We parameterize $f$ using Hermite polynomials.  The $n$-th Hermite polynomial takes the form
\begin{equation}
\label{eqn:hermdef}
H_n(x) = (\sqrt{2 \pi} n!)^{-1/2} (-1)^n e^{x^2/2} \dfrac{\mathrm{d}^n}{\mathrm{d}x^n} e^{-x^2/2}
\end{equation}
Let $\langle f, g \rangle_w = \int_{\mathbb{R}} f(x) g(x) \exp(-x^2/2) \, dx$ denote a weighted $L^2$ inner product.  Then, $\langle H_i, H_j \rangle_w = \delta_{ij}$, i.e., the Hermite polynomials are orthonormal with respect to the weighted inner product.  In fact, with respect to this inner product, the Hermite polynomials form an orthonormal basis of $L^2_w(\mathbb{R}) = \{ f \, : \, \langle f, f \rangle_w < \infty \}$.

Now let $\alpha = (\alpha_1, \ldots, \alpha_d) \in \mathbb{Z}^d_+$ denote a multi-index.  We use the notation $|\alpha| = \sum_j \alpha_j$ and $x^\alpha = \prod_j (x_j)^{\alpha_j}$ for $x = (x_1, \ldots, x_d) \in \mathbb{R}^d$.  For $x \in \mathbb{R}^d$ and a multi-index $\alpha$, we also define
\begin{equation}
\label{eqn:hermmultiindex}
H_\alpha(x) = \prod_{j=1}^d H_{\alpha_j}(x_j).
\end{equation}
We write $f(x) = (f_1(x), \ldots f_d(x))$ and then parameterize each component:
\begin{equation}
\label{eqn:param1}
f_j(x) = \sum_{m=0}^M \sum_{|\alpha|=m} \beta^j_\alpha H_\alpha(x).
\end{equation}
We see that the maximum degree of $H_\alpha(x)$ is $|\alpha|$.  Hence we think of the double sum in (\ref{eqn:param1}) as first summing over degrees and then summing over all terms with a fixed maximum degree.  We say maximum degree because, for instance, $H_2(z) = (z^2-1)/(\sqrt{2 \pi} 2)^{1/2}$ contains both degree $2$ and degree $0$ terms.

There are $\binom{m + d - 1}{d-1}$ possibilities for a $d$-dimensional multi-index $\alpha$ such that $|\alpha| = m$.  Summing this from $m=0$ to $M$, there are $\widetilde{M} = \binom{M+d}{d}$ total multi-indices in the double sum in (\ref{eqn:param1}).  Let $(i)$ denote the $i$-th multi-index according to some ordering.  Then we can write
\begin{equation}
\label{eqn:param2}
f_j(x) = \sum_{i=1}^{\widetilde{M}} \beta^j_{(i)} H_{(i)}(x).
\end{equation}

\paragraph{Data.} We consider our data $\mathbf{x} = \{x_j\}_{j=0}^L$ to be direct observations of $X_t$ at discrete points in time $\mathbf{t} = \{t_j\}_{t=0}^L$.  Note that these time points do not need to be equispaced.  

To achieve our estimation goal, we apply expectation maximization (EM).  We regard $\mathbf{x}$ as the incomplete data.  Let $\Delta t = \frac{1}{L} \sum_{j=1}^{L} (t_j - t_{j-1})$ be the average interobservation spacing.  We think of the missing data $\mathbf{z}$ as data collected at a time scale $h \ll \Delta t$ that is fine enough such that the transition density of (\ref{eqn:sde}) is approximately Gaussian.  To see how this works, let $\mathcal{N}(\mu, \Sigma)$ denote a multivariate normal with mean vector $\mu$ and covariance matrix $\Sigma$.  Now discretize (\ref{eqn:sde}) in time via the Euler-Maruyama method with time step $h > 0$; the result is
\begin{equation} \label{eqn:euler}
\widetilde{X}_{n+1} = \widetilde{X}_n + f(\widetilde{X}_n) h + h^{1/2} \Gamma Z_{n+1},
\end{equation}
where $Z_{n+1} \sim \mathcal{N}(0, I)$ is a standard multivariate normal, independent of $X_n$.  Note that $\widetilde{X}_{n+1} | \widetilde{X}_n = v$ has a $\mathcal{N}(v + f(v) h, h \Gamma^2)$ distribution.  As $h$ decreases, this Gaussian will converge to the true transition density $X_{(n+1)h} | X_{nh} = v$, 
where $X_t$ refers to the solution of (\ref{eqn:sde}).

\paragraph{Diffusion Bridge.} To augment or complete the data, we employ diffusion bridge sampling, using a Markov chain Monte Carlo (MCMC) method that goes back to [Roberts and Stramer, 2001; Papaspiliopoulos, Roberts, and Stramer, 2013].  Let us describe this method here.  We suppose our current estimate of $\btheta = (\beta, \gamma)$ is given.  Then the goal is to generate a sample path of (\ref{eqn:sde}) conditioned on both the initial value $x_i$ at time $t_i$, and the final value $x_{i+1}$ at time $t_{i+1}$.  By a sample path, we mean $F-1$ \emph{new} samples $\{z_{i,j}\}_{j=1}^{F-1}$ at times $t_i + j h$ with $h = (t_{i+1} - t_i)/F$.

To generate such a path, we start by drawing a sample from a Brownian bridge with the same diffusion as (\ref{eqn:sde}).  That is, we sample from the SDE
\begin{equation}
\label{eqn:bbridgesde}
d\widehat{X}_t = \Gamma dW_t
\end{equation}
conditioned on $\widehat{X}_{t_i} = x_i$ and $\widehat{X}_{t_{i+1}} = x_{i+1}$.  This Brownian bridge can be described explicitly:
\begin{equation}
\label{eqn:bbridge}
\widehat{X}_t = \Gamma W_{t - t_i} + x_i - \frac{t - t_i}{t_{i+1} - t_i} (\Gamma W_{t_{i+1} - t_i} + x_{i} - x_{i+1} )
\end{equation}


Let $\mathbf{z}^{(r)}$ denote the $r^\text{th}$ diffusion bridge sample path:
\begin{equation}
z^{(r)} \sim z \, | \, x, \beta^{(k)}
\end{equation}
The observed and sampled data can be interleaved together to create a time series (completed data)
$$
\mathbf{y}^{(r)} = \{y_j^{(r)}\}_{j=1}^N
$$
of length $N = LF + 1$.  

%Specifically, the density is
%$$
%\left( \prod_{i=1}^d \frac{1}{\sqrt{2 \pi h \gamma_i^2}} \right)
%\exp \left( -\frac{1}{2h} (x - v - h \sum_{k=1}^M \beta_k \phi_k(v))^T \Gamma^{-2} (x - v - h \sum_{\ell=1}^M \beta_\ell \phi_\ell(v)) \right).
%$$

\paragraph{EM.} The EM algorithm consists of two steps, computing the expectation of the log likelihood function (on the completed data) and then maximizing it with respect to the parameters $\btheta = (\beta, \gamma)$.
\begin{enumerate}
\item Start with an initial guess for the parameters, $\btheta^{(0)}$.
\item For the expectation (or E) step,
\begin{equation}
\label{eqn:expectation}
Q(\btheta, \btheta^{(k)}) = \mathbb{E}_{\bz \mid \bx, \btheta^{(k)}} [\log p(\bx, \bz \mid \btheta)]
\end{equation}
Our plan is to evaluate this expectation via bridge sampling.  That is, we will sample from diffusion bridges $\bz \mid \bx, \btheta^{(k)}$.  Then $(\bx, \bz)$ will be a combination of the original data together with sample paths.
\item For the maximization (or M) step, we start with the current iterate and a dummy variable $\btheta$ and define
\begin{equation}
\label{eqn:maximization}
\btheta^{(k+1)} = \arg \max_{\btheta} Q(\btheta, \btheta^{(k)})
\end{equation}
It will turn out that we can maximize this quantity without numerical optimization.  All we will need to do is solve a least-squares problem.
\item Iterate Step 2 and 3 until convergence.
\end{enumerate}

\paragraph{Details.} With a fixed parameter vector $\btheta^{(k)}$, the SDE (\ref{eqn:sde}) is specified completely, i.e., the drift and diffusion terms have no further unknowns.  


Suppose we form $R$ such time series.  The expected log likelihood can then be approximated by
\begin{align*}
Q(\btheta, \btheta^{(k)}) &= \mathbb{E}_{\bz \mid \bx, \btheta^{(k)}} [\log p(\bx, \bz \mid \btheta)] \\
 &\approx \frac{1}{R} \sum_{r=1}^R \biggl[ \sum_{j=1}^N \left[ \sum_{i=1}^d -\frac{1}{2} \log (2 \pi h \gamma_i^2) \right] \\
 &\qquad -\frac{1}{2h} (y_j^{(r)} - y_{j-1}^{(r)} - h \sum_{k=1}^M \beta_k \phi_k(y_{j-1}^{(r)}))^T \Gamma^{-2} (y_j^{(r)} - y_{j-1}^{(r)} - h \sum_{\ell=1}^M \beta_\ell \phi_\ell(y_{j-1}^{(r)}) ) \biggr] 
\end{align*}

To maximize $Q$ over $\btheta$, we first assume $\Gamma = \opdiag \gamma$ is known and maximize over $\beta$.  This is a least squares problem.  The solution is given by forming the matrix
$$
\mathcal{M}_{k,\ell} = \frac{1}{R} \sum_{r=1}^{R} \sum_{j=1}^N h \phi_k^T (y_{j-1}^{(r)}) \Gamma^{-2} \phi_\ell^T (y_{j-1}^{(r)})
$$
and the vector
$$
\rho_k = \frac{1}{R} \sum_{r=1}^{R} \sum_{j=1}^N \phi_k^T (y_{j-1}^{(r)}) \Gamma^{-2} (y_j^{(r)} - y_{j-1}^{(r)}).
$$

We then solve the system
$$
\mathcal{M} \beta = \rho
$$
for $\beta$.  Now that we have $\beta$, we maximize $Q$ over $\gamma$.  The solution can be obtained in closed form:
$$
\gamma_i^2 = \frac{1}{R N h} \sum_{r=1}^{R} \sum_{j=1}^N (( y_j^{(r)} - y_{j-1}^{(r)} - h \sum_{\ell=1}^M \beta_\ell \phi_\ell (y_{j-1}^{(r)}) ) \cdot e_i )^2
$$
where $e_i$ is the $i^\text{th}$ canonical basis vector in $\mathbb{R}^d$.

We demonstrate the method for 1, 2 and 3 dimensional systems. 
\begin{itemize}
\item For the 1-dimensional system, we use the ? oscillator:
\begin{equation}
\mathrm{d}X(t) = (\alpha X(t) + \beta X(t)^2 + \gamma) \: \mathrm{d}t + g \: \mathrm{d}W(t)
\end{equation}
\item For the 2-dimensional system, we use the undamped Duffing oscillator:
\begin{align*}
\mathrm{d}X_1(t) & = X_2(t) \mathrm{d}t + g_1 \: \mathrm{d} W_1(t) \\
\mathrm{d}X_2(t) & = (-X_1(t) - X^3_1(t)) \mathrm{d}t + g_2 \: \mathrm{d} W_2(t)
\end{align*}
\item For the 3-dimensional case, we consider 2 different form of equations. The first one is the damped Duffing oscillator, a general form of the damped oscillator considered in the 2-dimensional case:
\begin{align*}
\mathrm{d}X_1(t) & = X_2(t) \: \mathrm{d}t + g_1 \: \mathrm{d}W_1(t) \\
\mathrm{d}X_2(t) & = (\alpha X_1(t) - \beta X_1(t) - \delta X_2(t) + \gamma \cos (X_3(t))) \: \mathrm{d}t + g_2 \: \mathrm{d}W_2(t) \\
\mathrm{d}X_3(t) & = \omega \: \mathrm{d}t + g_3 \: \mathrm{d}W_3(t)
\end{align*}
\item Another example considered for the 3-dimensional case is the Lorenz oscillator:
\begin{align*}
\mathrm{d}X_1(t) & = \sigma (X_2(t) - X_1(t)) \: \mathrm{d}t + g_1 \: \mathrm{d}W_1(t) \\
\mathrm{d}X_2(t) & = (X_1(t) (\rho - X_3(t))) \mathrm{d}t + g_2 \: \mathrm{d}W_2(t) \\
\mathrm{d}X_3(t) & = (X_1(t) X_2(t) - \beta X_3(t)) \: \mathrm{d}t + g_3 \: \mathrm{d}W_3(t)
\end{align*}
\end{itemize}
For simplicity, consider the example where the $X \in \mathbb{R}^2$ and the highest degree of the Hermite polynomial is three, including four Hermite polynomials:
\begin{align*}
f(x_1, x_2) & = \sum_{m = 0}^{2} \sum_{i+j = 0}^{i+j = m} \zeta_{i,j} \: \psi_{i,j} \\
& = \sum_{d = 0}^{3} \sum_{i + j = 0}^{i + j = 3} \zeta_{i, j} H_i(x_1) H_j(x_2) \\
& = \sum_{i + j = 0} \zeta_{i, j} H_i(x_1) H_j(x_2) + \sum_{i + j = 1} \zeta_{i, j} H_i(x_1) H_j(x_2) + \sum_{i + j = 2} \zeta_{i, j} H_i(x_1) H_j(x_2) + \sum_{i + j = 3} \zeta_{i, j} H_i(x_1) H_j(x_2) \\
& = \zeta_{0, 0} H_0(x_1)H_0(x_2) + \zeta_{0, 1}H_0(x_1)H_1(x_2) + \zeta_{1, 0}H_1(x_1)H_0(x_2) + \zeta_{0, 2}H_0(x_1)H_2(x_2) \\ & + \zeta_{2, 0}H_2(x_1)H_0(x_2) + \zeta_{1, 1}H_1(x_1)H_1(x_2) + \zeta_{0, 3}H_0(x_1)H_3(x_2) + \zeta_{3, 0} H_3(x_1)H_0(x_2) \\ & + \zeta_{2, 1} H_2(x_1)H_1(x_2) + \zeta_{1, 2} H_1(x_1)H_2(x_2)
\end{align*}

\section{Expectation Maximization Steps}
The data provided is in the form of a time series, $X \in \mathbb{R}^d$ at regular time points $t_l, 0 \leq l \leq L$. 
Note: EM step in the sampling writeup.

\section{Brownian bridge sampling}
When the inter-observation time of the observed data $X$ is large, the expectation step becomes less accurate. To mitigate this problem, we can fill in the observed data with a Brownian bridge. We generate many samples of the $N-$dimensional Brownian bridge and accept-reject samples using the Metropolis-Hastings algorithm. The approximation of the likelihood is obtained using the Girsanov likelihood function.

\subsection{Brownian bridge}
The $\mathbb{R}^N$ dimensional Brownian bridge is defined by the integral:
\begin{equation}
I(t) = \int_{0}^{t} \dfrac{1 - t}{1 -T} \: \mathrm{d} W(t)
\end{equation}
\subsection{Metropolis Algorithm}

\end{document}
